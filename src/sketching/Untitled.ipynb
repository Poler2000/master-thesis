{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f65be7af-685c-453a-b5bc-cc3f4af648a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, classification_report\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.utils import shuffle as skshuffle\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "14280ed4-63a0-4316-b9de-7833884871cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.io import loadmat\n",
    "from scipy.io import savemat\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import cdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a5d6d328-4504-4aac-9741-75c4b39050b7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 0)\t1.0\n",
      "  (2, 0)\t1.0\n",
      "  (3, 0)\t1.0\n",
      "  (4, 0)\t1.0\n",
      "  (5, 0)\t1.0\n",
      "  (7, 0)\t1.0\n",
      "  (8, 0)\t1.0\n",
      "  (9, 0)\t1.0\n",
      "  (11, 0)\t1.0\n",
      "  (14, 0)\t1.0\n",
      "  (15, 0)\t1.0\n",
      "  (18, 0)\t1.0\n",
      "  (19, 0)\t1.0\n",
      "  (20, 0)\t1.0\n",
      "  (21, 0)\t1.0\n",
      "  (22, 0)\t1.0\n",
      "  (23, 0)\t1.0\n",
      "  (24, 0)\t1.0\n",
      "  (25, 0)\t1.0\n",
      "  (28, 0)\t1.0\n",
      "  (30, 0)\t1.0\n",
      "  (31, 0)\t1.0\n",
      "  (32, 0)\t1.0\n",
      "  (33, 0)\t1.0\n",
      "  :\t:\n",
      "  (12806, 1)\t1.0\n",
      "  (12808, 1)\t1.0\n",
      "  (12809, 1)\t1.0\n",
      "  (12810, 1)\t1.0\n",
      "  (12811, 1)\t1.0\n",
      "  (12812, 1)\t1.0\n",
      "  (12815, 1)\t1.0\n",
      "  (12817, 1)\t1.0\n",
      "  (12818, 1)\t1.0\n",
      "  (12820, 1)\t1.0\n",
      "  (12821, 1)\t1.0\n",
      "  (12823, 1)\t1.0\n",
      "  (12827, 1)\t1.0\n",
      "  (12829, 1)\t1.0\n",
      "  (12830, 1)\t1.0\n",
      "  (12831, 1)\t1.0\n",
      "  (12832, 1)\t1.0\n",
      "  (12833, 1)\t1.0\n",
      "  (12834, 1)\t1.0\n",
      "  (12835, 1)\t1.0\n",
      "  (12837, 1)\t1.0\n",
      "  (12838, 1)\t1.0\n",
      "  (12839, 1)\t1.0\n",
      "  (12840, 1)\t1.0\n",
      "  (12841, 1)\t1.0\n",
      "<class 'scipy.sparse._csc.csc_matrix'>\n",
      "<class 'numpy.ndarray'>\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mat = loadmat('./results_dblp_4.mat')\n",
    "features_matrix = mat['embs']\n",
    "#print(type(features_matrix))\n",
    "#print(features_matrix[:10,:10])\n",
    "\n",
    "mat = loadmat(\"../data/dblp.mat\")\n",
    "A = mat[\"network\"]\n",
    "#graph = sparse2graph(A)\n",
    "labels_matrix = mat[\"group\"]\n",
    "labels_count = labels_matrix.shape[1]\n",
    "\n",
    "print(labels_matrix)\n",
    "\n",
    "print(type(labels_matrix))\n",
    "\n",
    "groups = labels_matrix.toarray()\n",
    "\n",
    "print(type(groups[:5, :5]))\n",
    "print(groups[:5, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6d7aa3b4-c3dc-4a59-8e1f-a4eb32f65c83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TopKRanker(OneVsRestClassifier):\n",
    "    def predict(self, gram_test, top_k_list):\n",
    "        assert gram_test.shape[0] == len(top_k_list)\n",
    "        probs = np.asarray(super(TopKRanker, self).predict_proba(gram_test))\n",
    "        all_labels = []\n",
    "        for i, k in enumerate(top_k_list):\n",
    "            probs_ = probs[i, :]\n",
    "            labels = self.classes_[probs_.argsort()[-k:]].tolist()\n",
    "            all_labels.append(labels)\n",
    "        return all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f78dac11-ad47-4758-8582-a7512c80f479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 1.]\n",
      " [1. 0.]]\n",
      "Overall Accuracy: 0.49\n",
      "Average Recall: 1.00\n",
      "Average F1 Score: 1.00\n",
      "\n",
      "Classification Report for each group:\n",
      "\n",
      "Group 0 Classification Report:\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_groups):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGroup \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Classification Report:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 38\u001b[0m     \u001b[38;5;28mprint\u001b[39m(classification_report(y_test[:, i], preds[:, i]))\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Example data creation\n",
    "n = 13326  # Number of items\n",
    "num_groups = 2  # Number of groups\n",
    "#groups = np.random.randint(0, 2, size=(n, num_groups))  # Random binary groups matrix\n",
    "print(type(groups))\n",
    "#groups = sparse.random(n, num_groups, density=0.2, format='csr', data_rvs=np.ones).toarray()  # Sparse binary groups matrix with 20% density\n",
    "features = np.random.rand(n, 128)  # Random features in 128 columns\n",
    "\n",
    "print(groups[:5, :5])\n",
    "\n",
    "# Creating DataFrame for features\n",
    "df_features = pd.DataFrame(features, columns=[f'feature_{i}' for i in range(128)])\n",
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_features, groups, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = TopKRanker(SVC(kernel=\"precomputed\",cache_size=4096,probability=True))\n",
    "gram = squareform( 1 - pdist(X_train, 'hamming'));\n",
    "gram_test = 1 - cdist(X_test, X_train, 'hamming');\n",
    "\n",
    "clf.fit(gram, y_train)\n",
    "\n",
    "top_k_list = [len(l) for l in y_test]\n",
    "preds = clf.predict(gram_test, top_k_list)\n",
    "mlb = MultiLabelBinarizer(classes=range(2))\n",
    "f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average='micro')\n",
    "\n",
    "accuracy = accuracy_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), normalize=False) / y_test.size\n",
    "recalls = recall_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=None)  # Returns recall for each class\n",
    "f1s = f1_score(mlb.fit_transform(y_test), mlb.fit_transform(preds), average=None)  # Returns F1 score for each class\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Average Recall: {np.mean(recalls):.2f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1s):.2f}\")\n",
    "print(\"\\nClassification Report for each group:\")\n",
    "for i in range(num_groups):\n",
    "    print(f\"\\nGroup {i} Classification Report:\")\n",
    "    print(classification_report(y_test[:, i], preds[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36fc692b-e480-4120-bcc0-b06232924d19",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Accuracy: 0.26\n",
      "Average Recall: 0.53\n",
      "Average F1 Score: 0.47\n",
      "\n",
      "Classification Report for each group:\n",
      "\n",
      "Group 0 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.47      0.11      0.18      1752\n",
      "         1.0       0.57      0.90      0.70      2246\n",
      "\n",
      "    accuracy                           0.56      3998\n",
      "   macro avg       0.52      0.51      0.44      3998\n",
      "weighted avg       0.52      0.56      0.47      3998\n",
      "\n",
      "\n",
      "Group 1 Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.55      0.85      0.67      2191\n",
      "         1.0       0.47      0.16      0.24      1807\n",
      "\n",
      "    accuracy                           0.54      3998\n",
      "   macro avg       0.51      0.51      0.46      3998\n",
      "weighted avg       0.52      0.54      0.48      3998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Model setup - MultiOutput with Logistic Regression\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Converting sparse matrix to dense for evaluation if necessary (depends on your specific evaluation function)\n",
    "y_test_dense = y_test#.toarray()\n",
    "y_pred_dense = y_pred#.toarray()\n",
    "\n",
    "# Evaluating the model using example metrics (adjust according to your specific needs)\n",
    "accuracy = accuracy_score(y_test_dense, y_pred_dense, normalize=False) / y_test_dense.size\n",
    "recalls = recall_score(y_test_dense, y_pred_dense, average=None)  # Returns recall for each class\n",
    "f1s = f1_score(y_test_dense, y_pred_dense, average=None)  # Returns F1 score for each class\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Average Recall: {np.mean(recalls):.2f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1s):.2f}\")\n",
    "print(\"\\nClassification Report for each group:\")\n",
    "for i in range(num_groups):\n",
    "    print(f\"\\nGroup {i} Classification Report:\")\n",
    "    print(classification_report(y_test_dense[:, i], y_pred_dense[:, i]))\n",
    "#df_group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1bd1b6-dbbd-4df6-9cfa-0a79897fcbb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Splitting the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, num_groups:], df.iloc[:, :num_groups], test_size=0.3, random_state=42)\n",
    "\n",
    "# Model setup - MultiOutput with Logistic Regression\n",
    "model = MultiOutputClassifier(LogisticRegression(max_iter=1000))\n",
    "\n",
    "# Model training\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluating the model using example metrics (adjust according to your specific needs)\n",
    "accuracy = accuracy_score(y_test, y_pred, normalize=False) / y_test.size\n",
    "recalls = recall_score(y_test, y_pred, average=None)  # Returns recall for each class\n",
    "f1s = f1_score(y_test, y_pred, average=None)  # Returns F1 score for each class\n",
    "\n",
    "print(f\"Overall Accuracy: {accuracy:.2f}\")\n",
    "print(f\"Average Recall: {np.mean(recalls):.2f}\")\n",
    "print(f\"Average F1 Score: {np.mean(f1s):.2f}\")\n",
    "print(\"\\nClassification Report for each group:\")\n",
    "for i in range(num_groups):\n",
    "    print(f\"\\nGroup {i} Classification Report:\")\n",
    "    print(classification_report(y_test.iloc[:, i], y_pred[:, i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60866ec9-1e4c-4403-ba90-e24da0bd5b57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense ndarray from csc_matrix:\n",
      "[[1 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n",
      "  (0, 0)\t1\n",
      "  (1, 1)\t2\n",
      "  (2, 2)\t3\n",
      "  (3, 3)\t4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "# Create a small example csc_matrix\n",
    "data = np.array([1, 2, 3, 4])\n",
    "row_indices = np.array([0, 1, 2, 3])\n",
    "col_indices = np.array([0, 1, 2, 3])\n",
    "sparse_matrix = csc_matrix((data, (row_indices, col_indices)), shape=(4, 4))\n",
    "\n",
    "# Convert csc_matrix to a dense ndarray\n",
    "dense_array = sparse_matrix.toarray()\n",
    "\n",
    "print(\"Dense ndarray from csc_matrix:\")\n",
    "print(dense_array[:3, :3])\n",
    "print(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991ade7-7215-4565-b574-31f79ffda70a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

\chapter{Operowanie na szkicach danych}
    W niniejszym rozdziale przedstawimy szerzej operacje, które mogą być wykonywane na szkicach danych oraz skomentujemy ich praktyczne zastosowania. Omówimy także algorytmy ExpSketch i FastExpSketch, generujące szkice, na których możliwe jest wykonywanie między innymi działań teoriomnogościowych.

\section{Szkice danych}
    Jak już wspomniano w poprzednich rozdziałach, szkice danych to kompaktowe struktury, które pozwalają na efektywną reprezentację kluczowych informacji o dużych zbiorach danych. Są one najczęściej generowane przez algorytmy online, przetwarzające strumienie danych i wykorzystujące funkcje haszujące do przypisywania identycznym elementom strumienia tych samych, pseudolosowych wartości. Formalnie, możemy zdefiniować strumień danych jako multizbiór $\mathfrak{M} = (\mathbb{S},m)$, gdzie $\mathbb{S}$ to zbiór unikalnych elementów, a funkcja $m : \mathbb{S} \rightarrow \mathbb{N}_{\geq 1}$ wyznacza ich liczność w strumieniu.

    Szkice danych były początkowo wykorzystywane głównie do estymacji prostych statystyk, takich jak liczba unikalnych elementów, czy częstość ich występowania. Tego typu wiedza bywa wystarczająca w wielu zastosowaniach, ale w niektórych przypadkach może okazać się niewystarczająca. Użyteczniejsze są szkice przechowujące dodatkowe informacje o elementach, takie jak ich wagi, czy dodatkowe atrybuty, a także umożliwiające wykonywanie działań na wyznaczonych szkicach. Przykładowo, posiadając szkice dwóch zbiorów danych, przydatna byłaby możliwość efektywnego wyznaczenia szkicu ich sumy lub przecięcia.  

\subsection{ExpSketch}
    Szczególnie ciekawym przykładem algorytmu mającego na celu zwiększyć liczbę możliwych do wykonania operacji, a tym samym użyteczność szkicu, jest ExpSketch\cite{Lemiesz_2021}. Operuje on na elementach postaci $(i, \lambda_{i})$, gdzie $i$ jest indentyfikatorem elementu, a $\lambda_{i}$ jego wagą. Wynikiem jego działania jest natomiast wektor zanurzeń o pewnym ustalonym rozmiarze $m$. W praktyce ExpSketch może reprezentować wiele różnych cech zbioru danych poprzez dodanie dodatkowego wymiaru w zanurzeniach elementów. W takim wypadku zamiast jednej wagi $\lambda_i$ do każdego elementu $i$ przypisany jest wektor wag $\lambda_i = (\lambda_{i,1}, \lambda_{i,2}, \dots, \lambda_{i,d})$, a szkic elementu przyjmuje formę macierzy o rozmiarzy $d \times m$. Jednak w konktekście szkicowania grafów jedyna cecha, którą będziemy rozważać, jest związana z wagami krawędzi. Dlatego przedstawimy wersję algorytmu dla $d = 1$.

    Schemat działania algorytmu ExpSketch jest dość prosty. Przetwarza on elementy w strumieniu sekwencyjnie. Dla każdego elementu $(i, \lambda_{i})$ obliczane jest $m$ haszy:
    \[
        h(i || 1), h(i || 2), \dots, h(i || m),
    \]
    gdzie $h: \mathfrak{M} \rightarrow [0,1]$ jest funkcją haszującą, a $||$ oznacza konkatenację reprezentacji binarnych liczb o ustalonej długości. Każdy z haszy jest nastęnie przekształcany przy użyciu odwrotności funkcji rozkładu wykładniczego z parametrem $\lambda_i$:
    \[
        F^{-1}(u) = - \frac{\ln u}{\lambda_i}.
    \]
    Zgodnie z \textit{inverse transform sampling theorem}\cite{Devroye_1986a}, otrzymujemy w ten sposób zmienną losową o rozkładzie wykładniczym:

    \[
        E = - \frac{\ln(h(i || k))}{\lambda_i} \sim Exp(\lambda_i).
    \]
    Każda z otrzymanych w ten sposób wartości jest porównywana z odpowiadającą jej pozycją w szkicu i zapisywana, jeśli jest mniejsza. Zasadę działania algorytmu ilustruje pseudokod \ref{alg:exp_sketch}. Co istotne, na tak zdefiniowanych szkicach można w prosty sposób wykonywać operacje teoriomnogościowe. 

    \begin{algorithm}
        \caption{ExpSketch($\mathfrak{M}, m$)}\label{alg:exp_sketch}
        $M = (M_1, M_2, \dots, M_m) \gets (\infty, \infty, \dots, \infty)$\;
        \ForEach{$(i, \lambda_i) \in \mathfrak{M}$}{
            \ForEach{$k \in {1,2,\dots,m}$}{
                $U \gets h(i || k)$\;
                $E \gets - \ln(U / \lambda_i)$\;
                $M_k \gets \min{\{M_k, E\}}$\;
            }
        }
        \Return{$M$}
    \end{algorithm}

\subsection{Operacja teoriomnogościowe na szkicach}
    Rozważmy dwa zbiory $\mathbb{A}$ i $\mathbb{B}$ oraz odpowiadające im szkice: 
    \[
        A = (A_1, A_2, \dots, A_m) \quad \text{i} \quad B = (B_1, B_2, \dots, B_m).  
    \]
    Wiemy, że elementy $A_k$ i $B_k$ szkiców $A$ i $B$ reprezentują minimalne wartości zmiennych losowych o rozkładzie wykładniczym:
    \[
        A_k \sim Exp(|\mathbb{A}|_{w}) \quad \text{i} \quad B_k \sim Exp(|\mathbb{B}|_{w}), 
    \]
    gdzie $|\mathbb{S}|_{w} = \sum\limits_{i \in \mathbb{S}}\lambda_i$. 
    \subsubsection*{Suma}
    Chcąc otrzymać szkic $C$, gdzie $C_k \sim Exp(|\mathbb{A} \cup \mathbb{B}|_{w})$, możemy zastosować operację minimum na odpowiadających sobie elementach szkiców $A$ i $B$:    
    \[
        A \mathbin{\mathaccent\cdot\cup} B = (\min{\{A_1, B_1\}}, \min{\{A_2, B_2\}}, \dots, \min{\{A_m, B_m\}}).
    \]
    Taka kostrukcja jest oczywiście intuicyjna. Gdyby algorytm ExpSketch otrzymał na wejściu zbiór $\mathbb{A} \cup \mathbb{B}$, to każdy z elementów obu zbiorów zostałby wykorzystany do wygenerowania hasza, a wynikowy szkic zawierałby najmniejsze z otrzymanych wartości.
    
    Z kolei samą wartość $|\mathbb{A} \cup \mathbb{B}|_{w}$ możemy estymować jako:
    \[
        \hat{U}(A, B) = \frac{m - 1}{\sum\limits_{k = 1}^{m} \min{\{A_k, B_k\}}} .
    \]

    \subsubsection*{Ważone podobieństwo Jaccarda} 
    Podobieństwo Jaccarda to miara podobieństwa dwóch zbiorów, zdefiniowana jako stosunek liczby elementów wspólnych do liczby elementów w sumie zbiorów:
    \[
        J(\mathbb{A}, \mathbb{B}) = \frac{|\mathbb{A} \cap \mathbb{B}|}{|\mathbb{A} \cup \mathbb{B}|}.
    \]
    Jak pokazano w \cite{Lemiesz_2021}, 
    \[
        J_w(\mathbb{A}, \mathbb{B}) = Pr[A_k = B_k]. 
    \]
    Stąd można łatwo wyznaczyć estymator podobieństwa Jaccarda zbiorów $\mathbb{A}$ i $\mathbb{B}$ jako:
    \[
        \hat{J}_w(A, B) = \frac{1}{m} \sum\limits_{k = 1}^{m} \mathbbm{1}[A_k = B_k].  
    \]

    \subsubsection*{Przecięcie}
    Statystytki dotyczące przecięcia zbiorów można łatwo estymować na podstawie sumy i podobieństwa Jaccarda. Dla $|\mathbb{A} \cap \mathbb{B}|_{w}$ mamy:
    \[
        \hat{I}(A, B) = \hat{J}_w(A, B) \cdot \hat{U}(A, B).  
    \]

\subsection{FastExpSketch}
    Łatwo zauważyć, że ExpSketch dla każdego elementu w strumieniu wykonuje $m$ iteracji pętli, po jednej dla każdej wartości przechowywanej w szkicu. Pojedyncza iteracja polega na obliczeniu hasza, wygenerowaniu liczby losowej, obliczeniu wartości $E$ i zaktualizowaniu szkicu. Każda z tych operacji wykonywana jest w czasie stałym, co daje złożoność czasową $\Omega(m)$ dla pojedynczego elementu w strumieniu. W przypadku przetwarzania krawędzi grafu oznacza to łączną złożoność czasową $\Omega(m|E|)$, gdzie $|E|$ to liczba krawędzi w grafie. Taka liczba operacji może być w praktyce zbyt duża, dlatego rozważać będziemy zoptymalizowaną wersję algorytmu, FastEdgeSketch\cite{Lemiesz_2023}. Jego idea opiera się na wykorzystaniu twierdzenia\cite{Devroye_1986a}:
    \begin{twierdzenie}
        \label{theo:orderStatsExp}        
        Niech $E_1, E_2, \dots, E_m$ będą niezależnymi zmiennymi losowymi o rozkładzie wykładniczym. Oznaczmy przez:
        \[
            E_{(1)} \leq E_{(2)} \leq \dots \leq E_{(m)}  
        \]
        ich statystyki pozycyjne. Wtedy dla każdego $k \in \{1,2, \dots m\}$ zachodzi równość rozkładów:
        \[
            E_{(k)} \stackrel{d}{=} E_{(k - 1)} + \frac{E_k}{m - k + 1}.       
        \]
    \end{twierdzenie}

    W oryginalnym algorytmie ExpSketch, dla każdego elementu generowanych jest $m$ wartości $E_{1}, E_{2}, \dots, E_{m}$, które są następnie porównywane z analogicznymi wartościami w szkicu. Można zauważyć, że jeśli dla danego $k$ zachodzi:
    \[
        E_{k} > \max{\{M_1, M_2, \dots, M_m\}},     
    \]
    to oczywiście $E_{k}$ nie zostanie zapisane w szkicu. Ten fakt pozwala na zmniejsezenie liczby operacji wykonywanych przez algorytm, przy utrzymaniu identycznych wyników. Konkretnie, zamiast wartości $E_{1}, E_{2}, \dots$, generowane są ich statystyki pozycyjne $E_{(1)}, E_{(2)}, \dots$, dopóki nie zajdzie warunek $E_{(i)} > \max{\{M_1, M_2, \dots, M_m\}}$. Następnie wygenerowane wartości $E_{(1)}, E_{(2)}, \dots, E_{(i - 1)}$ są porównywane z wartościami w szkicu. 
    
    Działanie FastEdgeSketch ilustruje pseudokod \ref{alg:fast_exp_sketch}. Algorytm rozpoczyna się od inicjalizacji szkicu, oraz zmiennych pomocniczych. Dla każdego elementu w strumieniu ustawiamy zmienną $S$ przechowującą aktualną pozycyjną na $0$. W linijkach $9-11$ obliczamy kolejne statystyki pozycyjne, korzystając z twierdzenia\ref{theo:orderStatsExp} i zapisujemy je na losowej pozycji $j$ w szkicu, o ile są mniejsze od aktualnie zapisanej tam wartości. Pozycje w  szkicu wyznaczamy linijkach $14-16$ na podstawie losowej permutacji $P$, za pomocą kroków analogicznych do algorytmu Fischera-Yatesa. Cała procedura jest przerywana, jeśli wygenerowana wartość $S$ przekroczy aktualne maksimum $MAX$ wartości w szkicu. Na końcu wartość $MAX$ jest aktualizowana, o ile zaszła taka potrzeba.

    Rozważając złożoność algorytmu, możemy skupić się na liczbie porównań w linii $19$. Oznaczmy przez $C_i$ zmienną losową wyznaczającą liczbę takich porównań dla elementu $i$. Rozważymy twierdzenie, którego dowód można znależć w \cite{Lemiesz_2023}:
    \begin{twierdzenie}
        \label{theo:avgComp}        
        Dla $i \geq 2$, $\Lambda_{i} = \lambda_{1}, \lambda_{2}, \dots, \lambda_{i}$ oraz $r_i = \frac{\lambda_{i}}{\Lambda_{i - 1}}$, zachodzi:
        \[
            \mathbb{E}[C_i] = m \left( 1 - \prod_{j = 1}^{m} \frac{j}{j + r_i} \right).   
        \]
    \end{twierdzenie}
    W przypadku ogólnym wartość oczekiwana zależy więc od wag elementów w strumieniu. W dalszych rozważaniach przyjmiemy, że są one jednakowe, co ma uzasadnienie np. w przypadku grafów nieważonych. W takim przypadku wartość oczekiwana liczby porównań wynosi: 
    \[
        \mathbb{E}[C_i] = m \left( 1 - \prod_{j = 1}^{m} \frac{j}{j + \frac{1}{i - 1}} \right) .
    \]
    Korzystając z własności liczb Stirlinga pierwszego rodzaju, możemy pokazać, że:
    \[
        \lim_{i \to \infty} i \mathbb{E}[C_i] = m H_m \quad \text{i} \quad \mathbb{E}[C_i] \leq \frac{m H_m}{i},
    \]
    gdzie $H_m$ jest $m$-tą liczbą harmoniczną. Stąd otrzymujemy:
    \[
        \mathbb{E}[C] \leq \sum\limits_{i = 1}^{n} \frac{m H_m}{i} = m H_m H_n,
    \]
    gdzie $H_n = \ln n + \gamma + O(\frac{1}{n})$, więc wartość oczekiwana liczby porównań jest logarytmiczna względem liczby elementów w strumieniu.

\begin{algorithm}
    \caption{FastExpSketch($\mathfrak{M}, m$)}\label{alg:fast_exp_sketch}
    $permInit \gets (1,2,3,\dots, m)$\;
    $M = (M_1, M_2, \dots, M_m) \gets (\infty, \infty, \dots, \infty)$\;
    $MAX \gets \infty$\;
    \ForEach{$(i, \lambda_i) \in \mathfrak{M}$}{
        $S \gets 0$\;
        $updateMAX \gets false$\;
        $P \gets permInit$\;
        \ForEach{$k \in {1,2,\dots,m}$}{
            $U \gets h(i || k)$\;
            $E \gets - \ln(U / \lambda_i)$\;
            $S \gets S + E / (m - k + 1)$\;
            \uIf{$S > MAX$}{
                break\;
            }
            $r \gets RandomInteger([k, m], seed = i)$\;
            $swap(P[k], P[r])$\;
            $j \gets P[k]$\;
            \uIf{$M_j = MAX$}{
                $updateMAX \gets true$\;
            }
            $M_j \gets \min{\{M_j, S\}}$\;
        }
        \uIf{$updateMAX$}{
            $MAX \gets \max{\{M_1, \dots, M_m\}}$\;
        }
    }
    \Return{$M$}
\end{algorithm}

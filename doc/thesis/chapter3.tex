\chapter{Przegląd literatury}\label{asddassad}

    Analiza wielkich grafów, zwłaszcza w ostatnich latach, przeżywa ogromny rozwój, budząc zainteresowanie grup badaczy z całego świata. Postępy w tej dziedzinie są naturalną odpowiedzią na potrzebę przetwarzania coraz większych zbiorów danych. Badanie interakcji w sieciach społecznościowych, zarządzanie ruchem internetowym, czy monitorowanie ruchu samochodowego to tylko niektóre z kluczowych w dzisiejszej rzeczywistości zastosowań. Grafy dobrze sprawdzają się jako modele do reprezentowania złożonych relacji między encjami, dzięki czemu odgrywają kluczową rolę w przetwarzaniu i wydobywaniu skondensowanych informacji ze strumieniowanych danych. Wybrane do tych celów algorytmy i metodologie w znacznym stopniu zależą od struktury badanych grafów, a także charakteru zapytań, które chcemy rozpatrywać. W zależności od wymagań dotyczących złożoności czasowej i pamięciowej, dokładności odpowiedzi, a także konkretnych informacji, na których zachowaniu nam zależy, inne metody mogą okazać się najlepszym wyborem. Przykładowo, odpowiedź na pytania o najkrótsze ścieżki między wierzchołkami może wymagać zapamiętania dodatkowych informacji o strukturze grafu, a więc potencjalnie użycia bardziej wyrafinowanego podejścia niż w przypadku zapytań wyłącznie o istnienie danej krawędzi. 

    W niniejszym przeglądzie literatury zagłębiamy się w sferę analizy dużych grafów ze szczególnym uwzględnieniem metod opartych na szkicach danych. Badamy ewoluujący krajobraz technik, algorytmów i aplikacji w tej dziedzinie, rzucając światło na metodologie stosowane w celu sprostania nieodłącznym wyzwaniom stawianym przez strumieniowe przesyłanie danych grafowych. Poprzez analizę najnowszych osiągnięć, staramy się zapewnić wgląd w znaczenie i potencjał analizy strumieni grafów w rozwiązywaniu złożonych zadań analitycznych w różnych dziedzinach, a także sformułować ogólne wnioski i wskazówki co do wyboru odpowiedniej metody do danego zastosowania. Dla lepszego ustrukturyzowania wiedzy omawiane algorytmy i struktury podzielone zostały na kilka kategorii, odpisanych dokładnie w dalszej części niniejszego rozdziału. Należy jednak pamiętać, że w niektórych przypadkach podział ten jest nieco umowny, gdyż różne podejścia i pomysły nierzadko przenikają się i inspirują wzajemnie, prowadząc do syntetycznych rozwiązań. 

\section{\textcolor{red}{Szkice danych}}
    Analiza strumieni danych jest szeroką dziedziną, nieograniczającą się oczywiście wyłącznie do danych grafowych. Istnieje wiele bardziej ogólnych, uniwersalnych metod, na których podstawie można budować rozwiązania bardziej wyspecjalizowane do konkretnych zadań. Doskonałym przykładem są \textcolor{red}{szkice danych} (\emph{ang. Sketch synopses}). Są to kompaktowe struktury zaprojektowane z myślą o wykorzystaniu ograniczonej ilości pamięci, umożliwiając jednocześnie aproksymację różnych statystyk i zapytań dotyczących strumienia danych, takich jak zliczanie elementów, wyznaczanie mediany, czy wykrywanie wartości odstających. Utrzymując mały szkic o stałym rozmiarze, struktury te umożliwiają analizę strumieni danych w czasie rzeczywistym bez konieczności przechowywania wszystkich elementów strumienia, co zapewnia wysoką wydajność nawet przy analizowaniu ogromnych strumieni.
    
    Istotną w kontekście budowy dalszych rozwiązań strukturą danych jest szkic Count-Min\cite{Cormode_Muthukrishnan_2005}. W przeciwieństwie do wielu proponowanych wcześniej \textcolor{red}{szkiców} zaprojektowanych do badania konkretnej statystyki stanowi on dość uniwersalną strukturę, oferując wsparcie dla \textcolor{red}{kilku fundamentalnych zapytań. Konkretnie, wyrażając strumień danych jako $a = [a_1, a_2, \dots, a_n]$, gdzie $a_i$ oznacza liczbę wystąpień elementu $i$ w strumieniu, Count-Min może zwrócić aproksymacje następujących wartości:
    \begin{itemize}
        \item $\mathcal{Q}(i) = a_i$ -- częstość występowania pojedynczego elementu 
        \item $\mathcal{Q}(l,r) = \sum\limits_{i = l}^{r} a_i$ -- łączna liczba wystąpień zakresu elementów 
        \item $\mathcal{Q}(a,b) = a \cdot b = \sum \limits_{i = 1}^{n} a_i b_i$ -- iloczyn skalarny dwóch wektorów.
    \end{itemize}}
     Z kolei na ich podstawie można budować bardziej skomplikowane zapytania. Co ważne, Count-Min, choć zwraca przybliżone wyniki, to gwarantuje spełnienie pewnych założeń odnośnie dokładności. Konkretnie, wyraża się ją zazwyczaj w kontekście definiowanych przez użytkownika parametrów $\epsilon, \delta$, a więc wymagamy, aby błąd względny w odpowiedzi na zapytanie mieścił się w zakresie współczynnika $\epsilon$ z prawdopodobieństwem $\delta$. Count-Min opiera się na zastosowaniu dwuwymiarowej tablicy o wymiarach $w \times d$, gdzie $w = \lceil \frac{e}{\epsilon} \rceil$ oraz $d = \lceil \ln \frac{1}{\delta} \rceil$. Nietrudno zauważyć, że zależą one od wymaganej dokładności. Struktura dalej wykorzystuje $d$ niezależnych funkcji haszujących, mapujących elementy ze strumienia na kolumny tablicy. Każdy z napływających elementów wiąże się więc z aktualizacją jednej komórki w każdym wierszu tablicy. Wtedy, przykładowo częstotliwość elementu możemy aproksymować jako minimum z wartości odpowiadających mu komórek. 

    Do innych wartych wzmianki metod możemy zaliczyć np. Lossy Counting\cite{Manku_Motwani_2012}, ukierunkowany na wskazywanie szczególnie często występujących elementów. \textcolor{red}{Z kolei AMS\cite{Alon_Matias_Szegedy_1996} skupia się na aproksymowaniu momentów częstotliwości (\emph{ang. frequency moments}), a więc wartości postaci:
    \[
        F_k = \sum\limits_{i = 1}^{n} a_i^k,
    \]
    gdzie n jest liczbą unikalnych elementów, $a_i$ jest liczbą wystąpień elementu $i$ w strumieniu, a $k$ jest numerem danego momentu. Momenty te niosą ze sobą istotne informacje statystyczne o przetwarzanych danych. Przykładowo $F_0$ to po prostu liczba unikalnych elementów, $F_1$ wyznacza długość strumienia, a $F_2$ można traktować jako miarę powtarzalności danych (\emph{ang. repeat rate})\cite{Good_1982}}. 
    
    gSketch\cite{Zhao_Aggarwal_Wang_2011} stanowi jedną z pierwszych prób przeniesienia idei tych ogólnych metod do świata grafów. Obsługuje on proste zapytania dotyczących grafu, takie jak częstość występowania danej krawędzi w strumieniu lub gęstość wybranego podgrafu. Autorzy wychodzą od metody Count-Min, wykorzystując jej dwuwymiarową tablicę do składowania częstotliwości krawędzi. Dodatkowo, algorytm wykorzystuje próbkę testową do podziału zbioru krawędzi na podzbiory, bazując na ich częstotliwości w taki sposób, aby efektywnie wykorzystać dostępną pamięć, a następnie przetwarza właściwy strumień danych. To przetwarzanie wstępne daje cenny wgląd w charakterystykę grafu, ale należy pamiętać, \textcolor{red}{że wyznaczanie reprezentacyjnej dla danego zbioru próbki testowej nie zawsze jest trywialne. Szczególnie w kontekście strumieniowanych danych grafowych, jakiekolwiek istotne informacje o grafie mogą być niedostępne w momencie rozpoczęcia jego przetwarzania. Z kolei wybór podzioru krawędzi ze strumienia jako próbki może nie reprezentować całościowej charakterystyki grafu.} gSketch, podobnie jak Count-Min jest metodą stratną, a praktycznym wyzwaniem jest odpowiednie wyważenie parametrów tak, aby zachować balans między zużytą pamięcią a dokładnością wyników.
    
    Choć metody takie jak gSketch mogą efektywnie zapamiętywać informacje dotyczące częstotliwości występowania krawędzi, to tracą przy tym wiedzę o strukturze grafu. Przykładowo, trudno za ich pomocą odpowiedzieć, czy istnieje ścieżka między danymi dwoma wierzchołkami. Jedną z prób rozwiązania tego problemu jest struktura gMatrix\cite{Khan_Aggarwal_2016}. Wykorzystuje ona, podobnie jak gSketch, zasadę działania Min-Count. Jednak w tym wypadku tablica zliczająca elementy wzbogaciła się o trzeci wymiar. Konkretnie, długość i szerokość tablicy odpowiadają haszom wierzchołków, a jej głębokość związana jest ponownie z liczbą funkcji haszujących. Struktura może więc wspierać zapytania związane zarówno z krawędziami, jak i wierzchołkami. gMatrix wspiera również wykrywanie szczególnie często występujących krawędzi i wierzchołków, a więc tych, których częstotliwość przekracza dany parametr $F$. Wymaga to jednak, aby wybrane funkcje haszujące były odwracalne. Wtedy wystarczy wybrać komórki o odpowiednio dużych wartościach i obliczyć odwrotności haszy, aby odzyskać informacje o wierzchołkach. Podobnie, można rozważać osiągalność między wierzchołkami, wybierając krawędzie o częstotliwości występowania przekraczającej pewne $F$ i przetwarzając je używając tradycyjnych algorytmów. Zwłaszcza w przypadku grafów o nierównej gęstości, możemy w ten sposób znacznie ograniczyć liczbę badanych krawędzi, zachowując wciąż odpowiednie ograniczenia na prawdopodobieństwo błędu.

\section{MDL -- minimalna długość opisu}
    Kluczowym problemem w analizie wielkich grafów jest rozmiar danych. Zasadne wydaje się więc pytanie, czy sposób zapisu analizowanych grafów jest efektywny. W wielu przypadkach może się okazać, że sama próba zmiany modelu opisującego dane przynosi znaczne oszczędności w kwestii wykorzystanej pamięci. Metoda minimalnej długości opisu (MDL - \emph{ang. Minimum Description Length}) koncentruje się na znalezieniu najprostszego modelu, który najlepiej opisuje strukturę grafu. Techniki oparte na MDL identyfikują wzorce i kompresują graf, wybierając model, który minimalizuje całkowitą długość opisu modelu i danych przy danym modelu, ułatwiając w ten sposób wydajne przechowywanie, przesyłanie i analizę dużych grafów. Bardziej formalnie, dla danych $D$ i rodziny dostępnych modeli $MF$ szukamy takiego modelu $M \in MF$, który minimalizuje $L(M) + L(D|M)$, gdzie $L(M)$ i $L(D|M)$ oznaczają odpowiednio długość opisu modelu $M$ oraz zakodowanych w nim danych $D$. Wiele metod opartych na MDL kompresuje dane w sposób bezstratny, co jest niewątpliwą zaletą tego modelu. 

    Istnieje wiele bezstratnych metod kompresujących grafy do reprezentacji o mniejszym narzucie pamięciowym. Jednak większość z nich zakładała działanie na tradycyjnej postaci grafu, gdzie  dane są skończone i znane na wejściu. Rzeczywistość analizy strumieni grafowych wymaga jednak bardziej elastycznego podejścia. Jedną z pierwszych inkrementacyjnych metod kompresji grafu jest MoSSo\cite{Ko_Kook_Shin_2020}. Reprezentacja wyjściowa tej metody składa się ze zbioru superwęzłów, a więc zbiorów wierzchołków oraz superkrawędzi. Każda taka superkrawędź oznacza połączenie wszystkich wierzchołków z danego superwęzła z wierzchołkami drugiego superwęzła. Dodatkowo, częścią zapisu jest także zbiór korekt krawędzi. Mają one postać pary zbiorów $C = (C^{+}, C^{-})$, oznaczających krawędzie, które należy dodać i usunąć, aby otrzymać prawdziwe dane. W ogólności aktualizowanie struktury dla nowych krawędzi sprowadza się do przemieszczania wierzchołków między superwęzłami w taki sposób, aby zminimalizować długość zapisu. Oczywiście sprawdzenie wszystkich możliwości byłoby kosztowne, dlatego autorzy zakładają sprawdzanie za każdym razem pewnego losowego zbioru potencjalnych wierzchołków do przemieszczania, co pozwala na znaczną oszczędność czasu, przy zachowaniu zadowalającego zużycia pamięci.

    W opisanym wyżej MoSSo losowy wybór potencjalnych zmian jest sterowany parametrami takimi jak prawdopodobieństwo utworzenia nowego superwęzła i liczba wierzchołków, których przesunięcie należy rozważyć w każdej rundzie. Jeśli wybrane wartości parametrów są zbyt małe lub zbyt wielkie, może to negatywnie wpływać na czas działania lub skuteczność kompresji. Jednak optymalne ich dobranie dla nieznanych wcześniej danych może być niemożliwe. Problem ten zauważają autorzy SGS\cite{Ma_Liu_Yang_Yang_Li_2022}. Jest to metoda bezparametryczna, w każdym kroku rozważająca przemieszczenie jedynie wierzchołków indukujących obecnie rozważaną krawędź $(u,v)$. Dodatkowo SGS opiera się na obserwacji, że w takim wypadku wystarczy rozważyć superwęzły bazując na wierzchołkach znajdujących się w odległości co najwyżej dwóch kroków od $u$ lub $v$. Następnie najlepsza zmiana jest wyznaczana zachłannie, bazując na funkcji podobieństwa sąsiedztwa wierzchołka oraz superwęzła.

    Przykładem nieco odmiennego podejścia może być z kolei metoda GS4\cite{Ashrafi-Payaman_Kangavari_Hosseini_Fander_2020}. O ile ona również wykorzystuje koncept grupowania wierzchołków w superwęzły, o tyle robi to, bazując zarówno na strukturze grafu, jak i pewnym zbiorze atrybutów wierzchołków. Jest to przydatne w przypadku, gdy wierzchołki grafu reprezentują bardziej złożone informacje. Przykładowo, w przypadku analizy danych pochodzącychc z portali społecznościowych, wierzchołkiem może być użytkownik, a atrybutami jego imię, wiek, ulubione zwierzę z gromady stułbiopławów itd. Niektóre z tych danych mogą być ważniejsze niż inne. GS4 pozwala na ustalenie wag dla atrybutów, dzięki czemu jest rozwiązaniem bardziej elastycznym, pozwalającym na sterowanie tym, jakie cechy grafu będą zachowywane priorytetowo. W przeciwieństwie do omawianych wcześniej algorytmów GS4 jest jednak metodą stratną. Dla oszczędności czasu przechowywany graf nie jest aktualizowany dla każdej nowej krawędzi, a raczej w wypadku istotnych różnic.  

\section{Metody oparte na modyfikacji macierzy sąsiedztwa}
    Jednym z najbardziej popularnych i być może najprostszym koncepcyjnie sposobem reprezentacji grafu jest macierz sąsiedztwa. Jej wiersze i kolumny odpowiadają poszczególnym wierzchołkom, a w komórkach przechowywane są wagi krawędzi pomiędzy nimi, o ile takowe krawędzie istnieją. W przypadku grafów nieważonych może to być np. wartość logiczna indykująca istnienie krawędzi lub ustalona stała. Ten sposób reprezentacji ma niewątpliwe zalety takie jak prostota implementacji i, przede wszystkim, stały czas dostępu do wag krawędzi. Z tego powodu niezaskakujący jest pomysł zachowania ogólnej zasady działania macierzy sąsiedztwa, przy jednoczesnej próbie zmniejszenia jej rozmiaru.     

    Jedną z pierwszych realizacji tej idei jest struktura TCM\cite{Tang_Chen_Mitra_2016}. Ma ona postać macierzy o boku długości $m$, gdzie $m$ jest pewną stałą. Podobnie jak w klasycznej macierzy sąsiedztwa, w jej komórkach składowane są wagi krawędzi. Zasadniczą różnicą jest natomiast sposób wyznaczania rzędu i kolumny odpowiadających danej parze wierzchołków. Są one bowiem wyznaczane przez wynik funkcji haszującej $H : V \rightarrow [1..m]$. Czas obliczania hasza jest stały, a co za tym idzie, złożoność czasowa zapytań i dodawania nowych krawędzi również. Teoretyczna złożoność pamięciowa także jest stała i wynosi $O(m^2)$. W praktycznych zastosowaniach wybór $m$ zależy jednak często od liczby krawędzi i przejmuje się najczęściej $m$ rzędu $O(\sqrt{|V|})$. Dokładność rezultatów zależy od rozmiaru macierzy i może być niska ze względu na kolizje haszy. Łatwo zauważyć, że jeśli $m$ jest istotnie mniejsze od $|V|$ to może do nich dochodzić często, co powoduje traktowanie różnych krawędzi jako kolejnych instancji tego samego połączenia. Autorzy, świadomi tego ograniczenia, proponują zastosowanie kilku parami niezależnych funkcji haszujących i stworzenie na ich podstawie wielu szkiców grafu. Przykładowo, jeśli badaną zmienną jest suma wag kolejnych instancji krawędzi między danymi dwoma wierzchołkami, to algorytm może sprawdzić odpowiednie komórki dla wszystkich szkiców, a następnie zwrócić minimalną wartość. Podejście to pozwala na analizę większych grafów niż w przypadku pojedynczego szkicu, ale ostatecznie nie rozwiązuje całkowicie problemu. Użyteczność struktury TCM w bazowej formie jest dyskusyjna, stanowi ona jednak punkt wyjściowy dla bardziej zaawansowanych rozwiązań.

    Strukturą opartą na koncepcie podobnym do TCM jest \emph{Graph Stream Sketch} (GSS)\cite{Gou_Zou_Zhao_Yang_2019}. Celem autorów było stworzenie metody oferującej lepszą skalowalność dla wielkich grafów. Podobnie jak w TCM, funkcja haszująca mapuje zbiór wierzchołków na pewien mniejszy zbiór $M$-elementowy. \textcolor{red}{Wymiary skompresowanej macierzy sąsiedztwa wynoszą notomiast $m \times m$, $m < M$}. Główną zmianą jest wprowadzenie dodatkowych cech opisujących wierzchołki. Na podstawie hasza $H(v)$ wyznaczany jest  podpis wierzchołka $f(v) (0 \leq f(v) < F)$, gdzie $M = m \times F$ i $f(v) = H(v)\%F$, a także adres $h(v) = \lfloor \frac{H(v)}{F} \rfloor$. Adresy służą do wyznaczania rzędu i kolumny komórek. Komórki te mają postać krotki lub, bardziej obrazowo, kubełka, w którym przechowywana jest para podpisów wierzchołków tworzących krawędź oraz kumulatywna waga krawędzi. Przechowywanie podpisów w komórkach pozwala zredukować ryzyko kolizji haszy. Łatwo bowiem zauważyć, że nawet jeśli dwa różne wierzchołki mają taki sam adres, to istnieje duża szansa, że ich podpisy są różne. Z tego względu nowa krawędź jest dodawana do kubełka tylko w wypadku, gdy jest on pusty lub gdy istniejące w nim podpisy są zgodne z podpisami wierzchołków krawędź tą tworzących. W przeciwnym przypadku jest ona zapisywana w dodatkowym buforze, mającym postać listy sąsiedztwa pełnych haszy. Pozwala on na dodawanie nowych krawędzi z niskim ryzykiem kolizji, nawet jeśli sama macierz jest już zapełniona. Należy natomiast zauważyć, że część macierzowa struktury jest bardziej efektywna czasowo, oferując stały czas odpowiedzi na zapytanie, podczas gdy dla bufora jest on liniowy względem liczby wierzchołków. Dokładność odpowiedzi w części macierzowej zależy od długości podpisów. Potencjalnym problemem GSS jest niskie wykorzystanie pamięci w macierzy. Przy kolizji adresów nowe krawędzie mogą trafiać do bufora, mimo, że w samej macierzy pozostaje wiele pustych komórek. Aby temu zaradzić, autorzy proponują haszowanie krzyżowe (\emph{ang. square-hashing}). Zakłada ono obliczanie dla każdego wierzchołka sekwencji niezależnych adresów. Podczas wstawiania nowych krawędzi algorytm sprawdza nie jedną komórkę macierzy, a kilka, zgodnie z sekwencją adresów i wybiera pierwszą spełniającą wymagania co do zgodności podpisów. Istnieje probabilistyczne ograniczenie na błąd względny zapytań postaci $Pr(\tilde{f}(s,d) - f(s,d) / \bar{w} > \delta) \leq \frac{|E|}{\delta m^2 4^{f}}$, gdzie $\tilde{f}(s,d)$ jest zwróconą sumą wag krawędzi $(s,d)$, $f(s,d)$ jej rzeczywistą wartością, $\bar{w}$ średnią wagą krawędzi, a $f$ długością podpisu.

    \begin{figure}[htb]
        \centering % Centers the tikzpicture
        \begin{tikzpicture}
            % Graph
            \begin{scope}[>=stealth, % Arrow tip style
                every edge/.style={draw,->}, % Makes all edges arrows
                every node/.style={fill=white}]
              \node[circle, draw] (a) at (2,2) {a};
              \node[circle, draw] (b) at (0,0) {b};
              \node[circle, draw] (c) at (4,2) {c}; 
              \node[circle, draw] (d) at (0,2) {d}; 
              \node[circle, draw] (e) at (2,0) {e}; 
              \node[circle, draw] (f) at (4,0) {f}; 
              \node[circle, draw] (g) at (2,4) {g}; 
          
              \path[->] (a) edge node {5} (c)
              (a) edge node {1} (g)
              (a) edge node {1} (e)
              (a) edge node {1} (b)
              (a) edge node {1} (f)
              (b) edge node {1} (d)
              (c) edge node {2} (f)
              (d) edge node {2} (a)
              (d) edge node {1} (f)
              (e) edge node {2} (b)
              (f) edge node {3} (e);
            \end{scope}
            
            % Adjacency table
            \node [right=2.5cm of a] (adj_table) {
              \begin{tabular}{|c|c|c|c|c|c|c|c|}
                \hline
                Node & a & b & c & d & e & f & g \\ \hline
                H(v) & 2 & 15 & 5 & 28 & 10 & 18 & 5 \\ \hline
                h(v), f(v) & 0,2 & 1,7 & 0,5 & 3,4 & 1,2 & 2,2 & 0,5 \\ \hline
              \end{tabular}
            };
        \end{tikzpicture}
        \caption{\textcolor{red}{Graf wraz z odpowiadającymi . Przykład zaczerpnięty z \cite{Gou_Zou_Zhao_Yang_2019}.}}
        \label{fig:gss_graph}
        \end{figure} 

        \begin{figure}[htb]
            \centering
            \begin{tikzpicture}[scale=0.8, every node/.style={scale=0.8},legend node/.style={minimum height=5mm, minimum width=5mm, draw},
                legend text/.style={right}] 
              % Define row and column labels
              \foreach \x in {0,1,2,3} {
                  % Column labels
                  \node at (\x*2+1,8) {\x};
              }
              \foreach \y in {0,1,2,3} {
                  % Row labels
                  \node at (-1,6.75-\y*1.5) {\y};
              }

              \draw (0,6) rectangle (2,7.5);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (0.5,6.75) {2,5};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (1.5,6.75) {6};
              
              % Second cell
              \draw (2,6) rectangle (4,7.5);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (2.5,6.75) {2,7};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (3.5,6.75) {1};
              
              % Third cell
              \draw (4,6) rectangle (6,7.5);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (4.5,6.75) {2,2};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (5.5,6.75) {1};
              
              % Fourth cell
              \draw (6,6) rectangle (8,7.5);
            
              \draw (0,4.5) rectangle (2,6);
              \draw (2,4.5) rectangle (4,6);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (2.5,5.25) {2,7};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (3.5,5.25) {2};

              \draw (4,4.5) rectangle (6,6);
              \draw (6,4.5) rectangle (8,6);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (6.5,5.25) {7,4};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (7.5,5.25) {1};

              \draw (0,3) rectangle (2,4.5);
              \draw (2,3) rectangle (4,4.5);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (2.5,3.75) {2,2};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (3.5,3.75) {3};

              \draw (4,3) rectangle (6,4.5);
              \draw (6,3) rectangle (8,4.5);

              \draw (0,1.5) rectangle (2,3);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (0.5,2.25) {4,2};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (1.5,2.25) {2};

              \draw (2,1.5) rectangle (4,3);
              \draw (4,1.5) rectangle (6,3);
              \node[draw, circle, inner sep=2pt, fill=cyan] at (04.5,2.25) {4,2};
              \node[draw, circle, inner sep=2pt, fill=yellow] at (5.5,2.25) {1};
              \draw (6,1.5) rectangle (8,3);

                % Legend
                        \draw[rounded corners] (9,5.25) rectangle (10.5,6.75);
                        \node[draw, circle, inner sep=2pt, fill=green] at (9.75,6) {2};
                        \draw[rounded corners] (12,5.25) rectangle (14,6.75);
                        \node[draw, circle, inner sep=2pt, fill=green] at (12.5,6) {10};
                        \node[draw, circle, inner sep=2pt, fill=yellow] at (13.5,6) {1};
                        \draw[rounded corners] (9,2.25) rectangle (10.5,3.75);
                        \node[draw, circle, inner sep=2pt, fill=green] at (9.75,3) {5};
                        \draw[rounded corners] (12,2.25) rectangle (14,3.75);
                        \node[draw, circle, inner sep=2pt, fill=green] at (12.5,3) {18};
                        \node[draw, circle, inner sep=2pt, fill=yellow] at (13.5,3) {1};
          
                        \draw[->]        (10.5,6)   -- (12,6);
                        \draw[->]        (10.5,3)   -- (12,3);
                        \draw[->]        (9.75,5.25)   -- (9.75,3.75);

  \node at (7,0) (legend) {
    \begin{tikzpicture}
      % Legend entries
      \node[legend node, circle, fill=cyan] (l1) {f(s),f(d)}; % Example of rounded node
      \node[legend text] (l1l) at (l1.east) {Podpis krawędzi $<s,d>$};
      
      \node[legend node, circle, right=of l1l, fill=yellow] (l2) {w}; % Example of circle node
      \node[legend text] (l2l) at (l2.east) {Waga krawędzi};
      
      \node[legend node, circle, right=of l2l, fill=green] (l3) {H(v)}; % Example of a simple node
      \node[legend text] at (l3.east) {Hasz wierzchołka $v$};
    \end{tikzpicture}
  };

            \end{tikzpicture}
            \caption{\textcolor{red}{Struktura GSS dla grafu przedsawionego na rysunku \ref{fig:gss_graph}}. W tym przypadku krawędzie $<a,e>$ oraz $<c,f>$ są składowane w buforze, gdyż }
            \label{fig:gss_structure}
            \end{figure}



    Większość struktur służących podsumowujących strumieniowane grafy nie przechowuje informacji o czasie wystąpienia krawędzi. Nie wspierają one więc zapytań z zakresem czasowym, a więc np., czy dana krawędź wystąpiła w zakresie $[t, t + L)$. Tego typu zapytania mogą być kluczowe np. w przypadku analizy danych dotyczących rozprzestrzeniania się wirusów (TODO: Citation needed). Problem ten podejmuje praca proponująca strukturę Horae\cite{Chen_Zhou_Chen_Xiao_Jin_Li_2022}. W jej wypadku krawędź $e_i = (<s_i, d_i>, w_i, t_i)$ jest wstawiana do komórki o adresie $(h(s_i | \gamma(t_i)), h(d_i | \gamma(t_i)))$, gdzie $\gamma(t_i) = \lfloor \frac{t_i}{gl} \rfloor$ i $gl$ jest długością przedziałów czasowych. Intuicyjnie, zapytanie o pojawienie się krawędzi w zakresie czasowym $[T_b, T_e]$ moze być transformowane w sekwencję zapytań o pojedyncze zakresy, których wyniki są sumowane, a więc $Q([T_b, T_e]) = Q([T_b]), Q([T_{b+1}]), \dots Q([T_e])$. Jednak dla takiego algorytmu złożoność czasowa jest liniowa względem liczby zakresów. Autorzy starają sie poprawić ten aspekt, zauważajac, iż przedział długości $L$ może zostać zdekomponowany do co najwyżej $2\log{L}$ podprzedziałów podsiadających dwie szczególne cechy. Po pierwsze, wszystkie zakresy czasowe w danym podprzedziale mają wspólny prefiks binarny. Po drugie, prefiksy różnych podprzedziałów mają różne długości. Z tego względu Horae zapamiętuje $O(\log(T))$ identycznych skompresowanych macierzy, gdzie $T$ jest liczbą rozróżnialnych zakresów czasowych. Każda z nich jest utożsamiana z jedną warstwą struktury. Warstwy odpowiadają z kolei różnym długościom prefiksów. Dzięki temu zamiast wykonywać liniową względem długości przedziału czasowego liczbę zapytań, wystarczy zdekomponować przedział na podprzedziały i na ich podstawie wykonać co najwyżej jedno zapytanie na warstwę. 
    
    Metody oparte na macierzach w większości przypadków nie czynią założeń co do struktury grafu. Takie ogólne podejście oczywiście zapewnia wysoką uniwersalność, jednak w niektórych przypadkach może być nieefektywne. Przykładowo, jeśli wierzchołki w grafie są mocno zróżnicowane pod względem stopnia, a więc bardziej obrazowo, da się wyróżnić obszary gęste i rzadkie w grafie, to kolizje haszy mogą zdarzać się często. Struktura Scube\cite{Chen_Zhou_Chen_Jin_2022} używa probabilistycznego zliczania do identyfikacji wierzchołków wysokiego stopnia. Przeznaczane jest dla nich więcej kubełków w macierzy niż dla wierzchołków o niskich stopniach, co pozwala bardziej efektywnie zarządzać zapełnieniem macierzy. 

    Metody takie jak GSS czy Horae, choć często dają przyzwoite wyniki przy odpowiednim dobraniu parametrów do badanego grafu, to ostatecznie cierpią z uwagi na ograniczoną skalowalność. Jedną z prób odpowiedzi na ten problem jest struktura AUXO\cite{Jiang_Chen_Jin_2023}. Korzysta ona z macierzy przechowujących podpisy wierzchołków, podobnie jak GSS. Jednak, zamiast wstawiać nadmiarowe krawędzie do bufora o liniowym czasie dostępu, AUXO wykorzystuje wiele macierzy ustawionych w strukturę drzewa. Konkretnie, jest to binarne lub czwórkowe drzewo prefiksowe, w którego strukturę zaszyte zostały prefiksy podpisów wierzchołków. W ten sposób na każdym kolejnym poziomie drzewa podpisy przechowywane w komórkach mogą być coraz krótsze, gdyż informacja ta jest wbudowana w kształt struktury. Pozwala to osiągnąć logarytmiczny względem liczby krawędzi czas odpowiedzi na zapytania. Warto zauważyć, że złożoność pamięciowa jest ograniczona przez długość podpisów, która wyznacza maksymalną głębokość drzewa. Niemniej jednak liczba możliwych do przetworzenia krawędzi jest eksponencjalna w stosunku do liczby bitów podpisu, więc stosunkowo łatwo można dobrać wystarczające wartości. W praktycznych zastosowaniach autorzy wskazują, nieco niefortunnie, na złożoność pamięciową zbliżoną asymptotycznie do $O(|E|(1 - \log(E)))$. Jak widać, AUXO osiąga efektywność pamięciową i skalowalność kosztem zwiększenia złożoności czasowej, co może być potencjalną wadą tego rozwiązania.

    \begin{figure}[htb]
    \centering
    \begin{tikzpicture}[%
        node distance=1.5cm and 3cm,
        mynode/.style={draw, rectangle, minimum size=5mm},
        arrow/.style={-Stealth},
        level 1/.style={sibling distance=35mm, level distance=20mm},
        level 2/.style={sibling distance=20mm, level distance=20mm},
        label node/.style={draw=none, rectangle},
        every label/.style={text width=2cm, align=center}
    ]
    
    % Nodes
    \node[mynode] (a) {$M_{0}$}
        child {node[mynode] (b) {$M_{1}^{\emptyset,0}$} 
            child {node[mynode] (d) {$M_{2}^{0,0}$} edge from parent[arrow] node[left] {0}}
            child {node[mynode] (e) {$M_{2}^{1,0}$} edge from parent[arrow] node[right] {1}}
            edge from parent[arrow] node[left] {0} 
        }
        child {node[mynode] (c) {$M_{1}^{\emptyset,1}$} 
            child {node[mynode] (f) {$M_{2}^{0,1}$} edge from parent[arrow] node[left] {0}}
            child {node[mynode] (g) {$M_{2}^{1,1}$} edge from parent[arrow] node[right] {1}}
            edge from parent[arrow] node[right] {1}
        };
    
    % Level labels and arrows
    \node[label node] (label_0) [left=0.5cm of a] {<010,011>};
    \node[label node] (l1) [left=0.5cm of b] {<010,11>};
    \node[label node] (l2) [left=0.5cm of d] {<10,11>};
    
    % Connect the level labels with arrows
    \draw[arrow] (label_0) -- (l1);
    \draw[arrow] (l1) -- (l2);
    
    \end{tikzpicture}
    \caption{\textcolor{red}{Struktura GSS dla grafu przedsawionego na rysunku \ref{fig:gss_graph}}. W tym przypadku krawędzie $<a,e>$ oraz $<c,f>$ są składowane w buforze, gdyż }
    \label{fig:auxo_structure}
\end{figure}

\section{zanurzenia/Osadzenia/zanurzenia TODO: potwierdzić nazewnictwo}
    Kolejną, dość rozległą i rozwijającą się metodologią w dziedzinie analizy grafów są zanurzenia grafów (\emph{ang. graph embeddings}). Nazwa odnosi się do reprezentowania wierzchołków przez wektory cech, a więc bardziej obrazowo, zanurzania grafu w niskowymiarowych przestrzeniach wektorowych. Wektory te mogą zachowywać między innymi kluczowe informacje topologiczne związane z połączeniami danego wierzchołka z jego sąsiadami. Przyjmują one najczęściej wartości rzeczywiste, a podobieństwo między wektorami może być mierzone na różne sposoby, np.  za pomocą podobieństwa cosinusów (\emph{ang. cosine similarity}) lub odległości Hamminga\cite{Lian_Zheng_Zheng_Ge_Cao_Tsang_Xie_2018}. \textcolor{red}{W tym konktekście, przez podobieństwo lub bliskość wierzchołków rozumieć będziemy ich wzajemne położenie w grafie. W szczególności, podobieństwo pierwszego rzędu zależy od liczby i czasem również wagi bezpośrednich połączeń między wierzchołkami, podczas gdy bliskość drugiego rzędu bierze pod uwagę połączenia poprzez jeden wierzchołek pośredni.} Co istotne, tego typu reprezentacja pozwala na łatwe zastosowanie uczenia maszynowego do analizy danych. W ogólności zanurzenia są stosowane szczególnie często w takich zadaniach jak klasyfikacja węzłów, przewidywanie połączeń między nimi oraz rekonstrukcja grafu. Wśród algorytmów opartych na tej metodologii możemy wyróżnić kilka unikalnych podkategorii, cechujących się odmiennymi podejściami do tego, jak cechy są wybierane i przetwarzane. Praca \cite{Yang_Qu_Hussein_Rosso_Cudré-Mauroux_Liu_2023} stanowi stosunkowo aktualny i rozbudowany przegląd tego typu metod, ukazując przy okazji wpływ hiperparametrów na uzyskiwane wyniki, zwłaszcza dla metod opartych na faktoryzacji i próbkowaniu. \textcolor{red}{Dla tych dwóch kategorii rozwiązań, autorzy proponują też uogólnione algorytmy, których szczegółowe działanie może być łatwo regulowane za pomocą paramterów.} Autorzy dzielą się również pewnymi wskazówkami co do wyboru hiperparametrów dla konkretnych scenariuszy. 

    \subsection{Próbkowanie}
        Techniki oparte na próbkowaniu koncentrują się na wyborze reprezentatywnego zbioru par węzłów z grafu wejściowego, w celu uchwycenia jego podstawowych właściwości strukturalnych. Na ich podstawie określony model uczy się reprezentowania wierzchołków poprzez optymalizację stochastyczną, \textcolor{red}{często wykorzystując SGD (\emph{Stochastic gradient descent}) do iteracyjnego poprawiania rezultatów. Funkcja straty może przybierać różne postaci, ale w ogólności powinna ona zachowywać informacje o sąsiedztwie wierzchołków}. Zapewnienie zadowalającej jakości reprezentacji wyjściowej wymaga często próbkowania znacznej liczby par węzłów, a zatem wymaga dużych zasobów obliczeniowych, w szczególności czasu procesora.
            
        Znanym przykładem tego typu techniki jest DeepWalk\cite{Perozzi_Al-Rfou_Skiena_2014}, wywodzący się, jak sama nazwa wskazuje, z uczenia głębokiego (\emph{ang. deep learning}). Wykorzystuje on sekwencję krótkich, losowych spacerów po wierzchołkach grafu do zapamiętywania tzw. reprezentacji społecznej \textcolor{red}{(\emph{social representation})} wierzchołków grafu, zawierającej informacje o podobieństwie sąsiedztwa wierzchołków i przynależności do \textcolor{red}{większych klastrów}. DeepWalk wykorzystuje algorytm SkipGram\cite{mikolov2013efficient} do efektywnego wyznaczania wektorów cech. Co ciekawe, można na ten schemat działania patrzeć jak na uogólnienie metod przetwarzania języka naturalnego do przetwarzania języka losowych spacerów, traktowanych jako zdania.
        
        Node2Vec\cite{Grover_Leskovec_2016} rozwija ideę DeepWalk, wprowadzając dodatkowe parametry sterujące spacerem. Pozwala to na wybranie, jak szybko spacer może oddalać się od startowego wierzchołka, efektywnie regulując, jak bardzo spacer zbliżony jest do przeszukiwania wszerz lub przeszukiwania wgłąb. Odpowiednie regulowanie tych parametrów sprzyja lepszemu poznaniu struktury grafu, która mogłaby być trudna do uchwycenia przy zwykłym spacerze losowym. 
        
        Kolejną wartą uwagi metodą tego typu jest LINE\cite{Tang_Qu_Wang_Zhang_Yan_Mei_2015}. Opiera się on na badaniu bliskości zarówno pierwszego, jak i drugiego rzędu między wierzchołkami. Bliskość pierwszego rzędu zależy od wagi połączeń między wierzchołkami. Z kolei bliskość drugiego rzędu mierzy podobieństwo struktury sąsiedztwa dwóch wierzchołków. Wiąże się to z założeniem, że wierzchołki o podobnym sąsiedztwie z dużym proawdopodobieństwem są podobne do siebie. LINE próbkuje bezpośrednio pary węzłów z grafu, ucząc się zanurzeń oddzielnie dla bliskości pierwszego i drugiego rzędu, a następnie łącząc je razem. 
        
        VERSE\cite{Tsitsulin_Mottin_Karras_Müller_2018} jest uniwersalną metodą, zdolną dostosować się do dowolnej miary podobieństwa węzłów, co pozwala na lepsze jej dostosowanie do konkretnych przypadków. Trenuje ona sieć neuronową złożoną z jednej warstwy, próbkując pary węzłów i starając się zachować wybraną miarę podobieństwa.

    \subsection{Faktoryzacja}
        Metody opierające się na faktoryzacji zakładają dekompozycję reprezentacji macierzowej grafu w taki sposób, aby uchwycić ukryte relacje między węzłami w przestrzeni o niższym wymiarze. Ma to na celu zachowanie właściwości strukturalnych grafu, takich jak bliskość węzłów i struktura sąsiedztwa. Jednak koszt obliczeniowych faktoryzacji macierzy może być bardzo wysoki w przypadku dużych grafów, zarówno w kontekście wykorzystania pamięci, jak i czasu obliczeń.

        Jedną z istotniejszych metod opartych na faktoryzacji jest GraRep\cite{Cao_Lu_Xu_2015}. Wyróżnia się ona uwzględnieniem \textcolor{red}{nie tylko lokalnych, ale i globalnych informacji strukturalnych w zanurzeniach, mając na celu dokładniejsze uchwycenie charakterystyki grafu}. Wykorzystywany model \textcolor{red}{bada sąsiedztwo wierzchołków, rozważając różnej długości ścieżki pomiędzy nimi}. Czyni to poprzez manipulowanie globalnymi macierzami przejścia zdefiniowanymi na grafie, bez angażowania procesów próbkowania. Autorzy proponują \textcolor{red}{naprzemienne wykorzystanie różnych wartości długości ścieżek, a także definiują funkcję straty, zależną od tychże długości}. Ostateczna reprezentacja dla każdego wierzchołka ma charakter globalny i powstaje z połączenia wielu wyuczonych modeli. Wykorzystanym sposobem dekompozycji jest rozkład według wartości osobliwych, szerzej znany jako SVD (\emph{Singular Value Decomposition}). Sprowadza się on do przedstawienia macierzy $A$ w postaci 
        \[
            A = U \Sigma V^T,
        \]
        gdzie $U$ i $V$ - macierze ortogonalne i $\Sigma$ - macierz diagonalna.
         
        HOPE\cite{Ou_Cui_Pei_Zhang_Zhu_2016} działa na podobnej zasadzie, faktoryzując macierze odpowiadające sąsiadom w odległości co najwyżej $k$ od danego wierzchołka z wykorzystaniem SVD. Autorzy biorą sobie jednak za cel zachowanie asymmetrycznej przechodniości. Przedstawia ona korelacje między skierowanymi krawędziami, a więc, istnienie skierowanej ścieżki z $u$ do $v$ może sugerować istnienie skierowanej krawędź z $u$ do $v$.  Prawdopodobieństwo ich istnienia jest wyliczane na podstawie indeksu Katza\cite{Katz_1953}. \textcolor{red}{Indeks ten jest definiowany jako suma ważona ścieżek między dwoma wierzchołkami. Waga ścieżki jest natomiast wykładniczą funkcją jej długości.} 
        
        NetMF\cite{Qiu_Dong_Ma_Li_Wang_Tang_2018} jest rozwiązaniem syntetycznym, generalizującym  metody próbkujące takie, jak DeepWalk, Node2Vec i LINE. Autorzy zauważają, że sposób działania tych metod można przedstawić jako niejawne faktoryzacje macierzy. Na podstawie tej obserwacji wyznaczają oni jawne postaci \textcolor{red}{macierzy, które są aproksymowane i faktoryzowane przez omawiane algorytmy}. Dodatkowo, bazując na relacji między metodą DeepWalk i macierzy Laplace'a dla grafu (\emph{ang. Laplacian matrix}), autorzy proponują algorytm efektywnie aproksymujący jawną postać tej macierzy.
        
        Z kolei ProNE\cite{Zhang_Dong_Wang_Tang_Ding_2019} \textcolor{red}{jest wykorzystuje, składającą się z dwóch zasadniczych króków. Pierwszy z nich polega na sprowadzeniu generowania zanurzeń wierzchołków do faktoryzacji rzadkich macierzy. W drugim kroku uzyskane wyniki są poprawiane w celu lepszego ukazania globalnych zależności w grafie. Aby to osiągnąć, ProNE stosuje nierówność Cheegera wyższego rzędu\cite{Lee_Gharan_Trevisan_2014} do propagacji zanurzeń.} Jak twierdzą autorzy, taki sposób przeprowadzania obliczeń może istotnie zwiększyć wydajność w praktycznych zastosowaniach.

    \subsection{Metody oparte na sieciach neuronowych}
        \textcolor{red}{
        Większość algorytmów opisywanych w tym podrozdziale wykorzystuje sieci neuronowe w mniejszym lub większym stopniu. Jednak często nie stanowią one istoty rozwiązania. Przykładowo, mogą one być eksploatowane tylko na dalszym etapie rozwiązania, np. do samej klasyfikacji wierzchołków, podczas gdy właściwe wektory zanurzeń są generowane w inny sposób. Nic nie stoi jednak na przeszkodzie, aby i do tego zadania zaprząc moc sieci neuronowych. Istnieje wiele rozwiązań, które budują zaawansowane modele sieci, zdolne radzić sobie z danymi grafowymi, bez konieczności użycia kosztownych faktoryzacji macierzy, czy próbkowania.}

        \textcolor{red}{
        Jednym z przykładów tego podejścia jest SDNE\cite{Wang_Cui_Zhu_2016}. Jest to metoda wykorzystująca głęboki, wielowarstwowy model dostosowany do uczenia pół-nadzorowanego. Konkretnie, część nadzorowana bada sąsiedztwo pierwszego rzędu między wierzchołkami, ucząc się lokalnej struktury grafu. Z kolei w uczeniu nienadzorowanym, mającym na celu wychwycenie globalnych informacji, SDNE używa sąsiedztwa drugiego rzędu. Autorzy proponują również użycie autoenkodera, aby minimalizować błędy rekonstrukcji danych. Taki model pozwala na zachowanie kompleksowej struktury grafu.}
        
        \textcolor{red}{
        DVNE\cite{Zhu_Cui_Wang_Zhu_2018} wyróżnia się na tle innych rozwiązań tym, że generuje zanurzenia wierzchołków w przestrzeni Wassersteina. Dokładniej, autorzy wykorzystują dystans 2-Wassersteina jako miarę podobieństwa między rozkładami prawdopodobieństwa, a więc, w tym przypadku, wygenerowanymi zanurzeniami oraz prawdziwymi danymi. Miara ta dobrze zachowuje tranzytywność oraz może modelować niepewność wierzchołków, powiązaną z wariancją rozkładu. Proces uczenia postulowany w DVNE jest uczeniem nienadzorowanym.}
        
        \textcolor{red}{
        GCN\cite{DBLP:journals/corr/KipfW16} jest kolejnym przykładem uczenia pół-nadzorowanego. W tym przypadku autorzy przyglądają się głównie zadaniu klasyfikacji wierzchołków. W tym celu definiują sprytną zasadę propagacji w konwolucyjnej sieci neuronowej, pozwalającą na jej działanie bezpośrednio na macierzy sąsiedztwa. Inspiracją są w tym przypadku metody aproksymacji spektralnych splotów grafów.}
        
        \textcolor{red}{
        GraphSAGE\cite{DBLP:journals/corr/HamiltonYL17} wykorzystuje uczenie z nadzorem. Bierze pod uwagę nie tylko topologię grafu, ale także cechy wierzchołków. W przeciwieństwie do większości podobnych metod GraphSAGE nie generuje unikalnego wektora zanurzenia dla każdego wierzchołka. Zamiast tego, trenuje zbiór funkcji agregujących informacje o cechach z lokalnego sąsiedztwa węzła. Mogą one być potem stosowane dla zupełnie nowych wierzchołków.}
        
    \subsection{Szkice}
        Szczególnie istotne z perspektywy niniejszej pracy są techniki zanurzania oparte na szkicach danych. Ich głównym założeniem jest tworzenie kompaktowych szkiców dla oryginalnych wielowymiarowych danych, przy jednoczesnym zachowaniu pewnej miary podobieństwa między wierzchołkami. \textcolor{red}{Często stosowanym pomysłem jest generowanie reprezentacji wierzchołków w przestrzeni Hamminga\cite{Hamming_1950}, a więc wykorzystanie kodów binarnych}. Metody te opierają się na zastosowaniu zachowujących podobieństwo technik haszowania w celu generowania zanurzeń węzłów. Wykorzystywane przez nie haszowanie może być zależne albo niezależne od danych. W literaturze te warianty bywają również opisywane jako odpowiednio \emph{learning-to-hash} i \emph{locality sensitive hashing}\cite{wang2017survey}.  

        INH-MF\cite{Lian_Zheng_Zheng_Ge_Cao_Tsang_Xie_2018} to pierwsza tego typu technika zaproponowana dla problemu zanurzeń grafu. Wykorzystuje ona haszowanie zależne od danych i uczy się reprezentowania szkicu grafu poprzez faktoryzację macierzy, zachowując jednocześnie wysokopoziomowe informacje o bliskości węzłów w grafie. W celu poprawy wydajności procesu uczenia autorzy proponują także zastosowanie uczenia w podprzestrzeni Hamminga, który przy przetwarzaniu nowych krawędzi aktualizuje tylko niektóre części wyjściowej reprezentacji.
        
        Większość opisywanych w tym podrozdziale metod skupia się na zachowaniu informacji strukturalnych dotyczących badanego grafu. O ile w wielu scenariuszach jest to wystarczające, o tyle istnieją przypadki, gdy przydatne może się okazać bardziej elastyczne podejście do definiowania cech wierzchołków, które algorytm powinien brać pod uwagę przy wyznaczaniu zanurzeń. Naprzeciw tym wymaganiom wychodzi algorytm NetHash\cite{Wu_Li_Chen_Zhang_2018}. Korzysta on ze zrandomizowanej techniki haszowania MinHash. Za jej pomocą koduje płytkie drzewa, z których każde jest zakorzenione w wierzchołku grafu. Właściwe szkice powstają poprzez rekurencyjne szkicowanie drzewa od dołu do góry, a więc od predefiniowanych sąsiednich węzłów najwyższego rzędu do węzła głównego. W ten sposób kodowane są zarówno atrybuty, jak i informacje o strukturze każdego węzła. NetHash zachowuje w szczególności  informacje zawarte bliżej węzła głównego.
        
        GNN\cite{4700287} (\emph{Graph Neural Network}) jest popularną metodą w zadaniach związanych z użyciem uczenia maszynowego dla danych grafowych. Zakłada ona zastosowanie konwolucyjnej sieci neuronowej do pół-nadzorowanych zadań klasyfikacji węzłów, a więc takich wykorzystujących zarówno etykietowane, jak i nieetykietowane dane. Jednak jej efektywne wykorzystanie zazwyczaj wymaga znacznych zasobów obliczeniowych ze względu na konieczność uczenia się dużej liczby parametrów. Sprawia to, że algorytmy oparte na GNN mogą być niepraktyczne dla problemu przetwarzania ogromnych grafów. \#GNN\cite{Wu_Li_Luo_Nejdl_2021} modyfikuje GNN w celu znalezienia kompromisu między dokładnością a wydajnością. Autorzy proponują wprowadzenie zrandomizowanego haszowania do implementacji przekazywania wiadomości i przechwytywania  informacji o sąsiedztwie wysokiego rzędu w sieci GNN. 
        
        \subsubsection*{NodeSketch}
            Ważnym i interesującym, także w kontekście niniejszej pracy, krokiem w rozwoju analizy wielkich grafów z wykorzystaniem szkiców jest algorytm NodeSketch\cite{Yang_Rosso_Li_Cudre-Mauroux_2019}. Autorzy wykorzystują rekursywne szkicowanie niezależne od danych, aby tworzyć szkice, które dla każdego wierzchołka zachowują informacje o jego sąsiedztwie wysokiego rzędu. Bardziej konkretnie, zaczynają oni od przedstawienia idei spójnego ważonego próbkowania \textcolor{red}{(\emph{ang. consistent weighted sampling})}. 
        
            Niech $V^a, V^b \in \mathbb{R}_{+}^{D}$ - nieujemne wektory \textcolor{red}{$D$-elementowe}. Wtedy ich podobieństwo min-max, zwane też ważonym indeksem Jaccarda możemy zdefiniować jako 
            \begin{equation} \label{eq:sim_mm}  
                Sim_{MM}(V^a, V^b) = \frac{\sum \limits_{i = 1}^D min(V_i^{a}, V_i^{b})}{\sum \limits_{i = 1}^D max(V_i^{a}, V_i^{b})}.
            \end{equation}
            Możemy wobec tej wartości zastosować normalizację sum-to-one $\sum \limits_{i = 1}^D V_{i}^{a} = \sum \limits_{i = 1}^D V_{i}^{b} = 1$ i oznaczyć takie znormalizowane podobieństwo przez $Sim_{NMM}$. Jest to efektywny sposób mierzenia podobieństwa \textcolor{red}{pomiędzy wektorami nieujemenych liczb rzeczywistych}, co ukazano w \cite{10.1145/2783258.2783406}. \textcolor{red}{Przykładowy graf wraz z macierzą SLA oraz odpowiadającą jej macierzą znormalizowanych podobieństw min-max ukazuje rysunek \ref{node_sketch_sla}.} Zasadniczo spójne ważone próbkowanie sprowadza się do generowania próbek danych w taki sposób, aby prawdopodobieństwo wylosowania identycznych wartości dla obu wektorów było równe ich podobieństwu min-max. Próbki te są następnie traktowane jako szkic wektora wejściowego. Proponowany proces generowania próbek dla wektora \textcolor{red}{$V = [V_1, V_2, \dots, V_D]$} jest dość prosty. Zaczyna się on od wyboru funckji haszującej $h_j$, takiej, że $h_j(i) \sim Uniform(0,1)$. Za jej pomocą próbka $S_j$ wyznaczana jest jako:
            \begin{equation} \label{eq:node_sketch_1}  
                S_j = \mathop{argmin}_{i \in \{1,2,\dots, D\}} \frac{-\log h_{j}(i)}{V_i}.
            \end{equation}
            Wybierając $L$ $(L \ll D)$ niezależnych funkcji haszujących i generując na ich podstawie próbki otrzymujemy szkic $S$ o rozmiarze $L$ wektora $V$. Ponadto, wynikowe szkice zachowują własność:

            \begin{equation} \label{eq:node_sketch_2}  
                Pr[S_{j}^{a} = S_{j}^{b}] = Sim_{NMM}(V^a, V^b), j = 1,2,\dots,L.
            \end{equation}

            Na podstawie tego schematu, autorzy budują właściwy algorytm NodeSketch. Jego działanie rozpoczyna się od wygenerowania zanurzeń dla wierzchołków na podstawie sąsiedztwa pierwszego i drugiego stopnia. Wykorzystuje do tego macierz sąsiedztwa SLA (\emph{Self-Loop-Augmented}) grafu. Różni się ona od zwykłej macierzy tym, że zawiera połączenia z każdego wierzchołka do niego samego, co pozwala prawidłowo zachowywać sąsiedztwo pierwszego rzędu. Innymi słowy, jeśli $A$ jest oryginalną macierzą sąsiedztwa, to macierz SLA ma postać:
            \[
                \tilde{A} = I + A,
            \]
            gdzie $I$ jest macierzą jednostkową. zanurzenia k-tego rzędu są generowane rekurencyjnie na podstawie macierzy SLA i zanurzeń $(k - 1)$-wszego rzędu. Ważną cechą spójnego ważonego próbkowania jest jednostajność generowanych próbek, co oznacza, że prawdopodobieństwo wybrania wartości $i$ jest proporcjonalne do $V_i$, a więc 
            \[
                Pr(S_j = i) = \frac{V_i}{\color{red} \sum_{m} V_m}.  
            \] \textcolor{red}{Własności ta implikuje, że udział elementu $i$ w szkicu $S$ stanowi nieobciążony estymator $V_i$, z czego można wywnioskować, że poprzez empiryczną obserwację rozkładu elementów w szkicu można aproksymać wektor $V$.} 
            
            Bazując na powyższych obserwacjach, autorzy przechodzą do właściwego sformułowania algorytmu. Dla każdego węzła $r$, obliczany jest przybliżony wektor SLA k-tego rzędu. Odbywa się to poprzez połączenie oryginalnego wektora SLA z wektorem (k-1)-wszego rzędu z odpowiednią wagą:
            \begin{equation} \label{eq:node_sketch_3}  
                \tilde{V}_{i}^{r}(k) = \tilde{V}_{i}^{r} + \sum\limits_{n \in 	\Gamma(r)} \frac{\alpha}{L} \sum\limits_{j = 1}^{L} \mathbbm{1}_{[S_{j}^{n}(k - 1) = i]},
            \end{equation}
            gdzie $\Gamma(r)$ to zbiór sąsiadów wierzchołka $r$, a $S^{n}(k-1)$ jest szkicem (k-1)-wszego rzędu dla wierzchołka $n$. Dodatkowo parametr $\alpha$ \textcolor{red}{steruje tym, jaki wpływ ma na wagę połączenia między wierzchołkami ma rząd ich sąsiedztwa, czyli liczba wierzchołków pośrednich. Dzięki temu krótkie połączenia są traktowane z wyższym priorytetem niż te długości bliskiej $k$}. Następnie, szkic wierzchołka $r$ oznaczany jako $S^{r}(k)$ jest wyznaczany z równania \ref{eq:node_sketch_1}. \textcolor{red}{Ostatecznym rezultatem wykonania algorytmu jest szkic 
            \[
                S(k) = [S^1(k), S^2(k), \dots, S^{D}(k)]^{T}, S(k) \in \mathbb{R}_{+}^{D \times L}.
            \]} 
            Działanie algorytmu ukazuje pseudokod \ref{alg:node_sketch}.

            \begin{figure}[htb]
            \centering % Centers the tikzpicture
            \begin{tikzpicture}
                % Graph
                \begin{scope}[every node/.style={draw, circle}]
                  \node (1) at (0,0) {1};
                  \node (2) at (2,1) {2};
                  \node (3) at (0,2) {3}; 
                  \node (4) at (4,0) {4}; 
                  \node (5) at (4,2) {5}; 
              
                  \draw (1) -- (2) -- (3) -- (1); 
                  \draw (2) -- (4); 
                  \draw (2) -- (5); 
                \end{scope}
                
                % Adjacency table
                \node [right=2.5cm of 2] (adj_table) {
                  \begin{tabular}{|c|c|c|c|c|}
                    \hline
                    1 & 1 & 1 & 0 & 0 \\ \hline
                    1 & 1 & 1 & 1 & 1 \\ \hline
                    1 & 1 & 1 & 0 & 0 \\ \hline
                    0 & 1 & 0 & 1 & 0 \\ \hline
                    0 & 1 & 0 & 0 & 1 \\ \hline
                  \end{tabular}
                };

                % Adjacency table
                \node [right=0.5cm of adj_table] (snmm_adj_table) {
                    \begin{tabular}{|c|c|c|c|c|}
                      \hline
                      1.0 & 0.43 & 1.0 & 0.2 & 0.2  \\ \hline
                      0.43 & 1.0 & 0.43 & 0.25 & 0.25  \\ \hline
                      1.0 & 0.43 & 1.0 & 0.2 & 0.2  \\ \hline
                      0.2 & 0.25 & 0.2 & 1.0 & 0.33  \\ \hline
                      0.2 & 0.25 & 0.2 & 0.33 & 1.0  \\ \hline
                    \end{tabular}
                };
            \end{tikzpicture}
            \caption{\textcolor{red}{Przykładowy graf wraz z odpowiadającą mu macierzą SLA oraz macierzą wartości podobieństw min-max, obliczonych przy pomocy równania \ref{eq:sim_mm}. Można spostrzec, że np. dla wierzchołka $4$, jego sąsiedztwo jest najbardziej zbliżone do sąsiedztwa wierzchołka $5$, czemu odpowiada wartość $0.33$ podobieństwa min-max. Warto też zauważyć, że $Sim_{NMM}(V,V)$, czyli podobieństwo wierzchołka do siebie samego jest zawsze równe $1$.}}
            \label{fig:node_sketch_sla}
            \end{figure}

            \begin{algorithm}
                \caption{NodeSketch($\tilde{A},k,\alpha$)}\label{alg:node_sketch}
                \uIf{$k > 2$}{
                    $S(k-1) \gets NodeSketch(\tilde{A},k - 1,\alpha)$\;
                    \ForEach{rząd $r$ w $\tilde{A}$}{
                        $\color{red} \tilde{V}_{i}^{r}(k) \gets \tilde{V}_{i}^{r} + \sum\limits_{n \in 	\Gamma(r)} \frac{\alpha}{L} \sum\limits_{j = 1}^{L} \mathbbm{1}_{[S_{j}^{n}(k - 1) = i]}, i \in \{1,2, \dots, D\}$\;
                        $\color{red} S_{j}^{r}(k) \gets \mathop{argmin}_{i \in \{1,2,\dots, D\}} \frac{-\log h_{j}(i)}{\tilde{V}_{i}^{r}(k)}, j \in \{1,2, \dots, L\}$\;
                    }
                }
                \ElseIf{$k = 2$}{
                    \ForEach{rząd $r$ w $\tilde{A}$}{
                        $\color{red} S_{j}^{r}(2) \gets \mathop{argmin}_{i \in \{1,2,\dots, D\}} \frac{-\log h_{j}(i)}{\tilde{V}_{i}^{r}(2)}, j \in \{1,2, \dots, L\}$\;
                    }
                }
                \Return{$S(k)$}
            \end{algorithm}

        \subsubsection*{SGSketch}
            Choć NodeSketch okazuje się efektywny w wielu scenariuszach testowych, to nie da się ukryć, że jego możliwości, podobnie jak wielu omawianych wcześniej metod generowania zanurzeń wierzchołków, są ograniczone. Po pierwsze, nie bierze on pod uwagę wag krawędzi, a tylko ich istnienie. Tymczasem waga połączenia może nieść ze sobą cenne informacje. \textcolor{red}{Po drugie, zakłada on, że cała macierz sąsiedztwa SLA jest znana w chwili wywołania algorytmu. Fakt ten sprawia}, że NodeSketch nie jest przystosowany do działania w formie On-Line, a więc reagowania na zmiany w grafie, jak np. ustanawianie nowych krawędzi. Ta obserwacja, choć z pozoru nieco rozczarowująca, \textcolor{red}{może stanowić inspirację do budowy kolejnych, bardziej elastycznych rozwiązań}. 

            Jedną z prób stworzenia metody mogącej radzić sobie z ewoluującym strumieniem krawędzi jest, oparty na NodeSketch, algorytm SGSketch\cite{Yang_Qu_Yang_Wang_Cudre-Mauroux_2022}. Jego główna idea polega na stopniowym zapominaniu dawnych krawędzi i wykorzystaniu efektywnego, inkrementacyjnego mechanizmu aktualizacji wygenerowanych zanurzeń.
            
            Niech $SG$ będzie strumieniowanym grafem. Typowa implementacja stopniowego zapominania krawędzi \textcolor{red}{polega na obliczaniu jej wagi bazując na momencie wystąpienia w strumieniu. Starszym krawędziom przypisywane będą mniejsze wartości, zgodnie z prawem rozpadu naturalnego (\emph{ang. exponential decay})}. Możemy więc przyjąć wagę krawędzi, która pojawiła się w chwili $t$ jako $w_t = e^{\lambda(t_n - t)}$, gdzie $t_n$ jest momentem wystąpienia najnowszej krawędzi, a $\lambda$ jest parametrem sterującym szybkością zapominania. Stąd elementy macierzy sąsiedztwa wyznaczane są ze wzoru:
            \begin{equation} \label{eq:sg_sketch_1}  
                A_{i,j} = A_{j,i} = \mathop{max}_{t \leq t_n} e^{-\lambda (t_n - t)}\mathbbm{1}_{[(r_i, r_j)^{t} \in SG]}.
            \end{equation}
            Taka forma zapominania ma charakter globalny, co oznacza, że  waga wszystkich istniejących krawędzi zmniejsza się w wyniku przetwarzania każdej nowej krawędzi. Może to prowadzić do kosztownych i niepotrzebnych zmian dla wierzchołków znajdujących się daleko od nowych krawędzi. Dlatego autorzy proponują zastosowanie lokalnej techniki zmniejszania wag. Formalnie, aby obliczyć wagę krawędzi $(r_i, r_j)$ zaobserwowanej w chwili $t$ dla wierzchołka $r_i$, wyznaczamy najpierw liczbę krawędzi wchodzących i wychodzących z $r_i$ w przedziale czasowym $(t,t_n]$: 
            \begin{equation} \label{eq:sg_sketch_2}  
                \phi_{(t,t_n]}(r_i) = \vert \{(r_p, r_q)^{t'} \vert r_i \in (r_p, r_q), t' \in (t,t_n] \} \vert.
            \end{equation}
            Ostatecznie, elementy macierzy mają postać:
            \begin{equation} \label{eq:sg_sketch_3}  
                A_{i,j} = \mathop{max}_{t \leq t_n} e^{-\lambda \phi_{(t,t_n]}(r_i)}\mathbbm{1}_{[(r_i, r_j)^{t} \in SG]}.
            \end{equation}

            Działanie właściwego algorytmu SGSketch rozpoczyna się od wygenerowania początkowego szkicu. Odbywa się to sposób w zasadzie identyczny jak w algorytmie NodeSketch, z tą różnicą, że wykorzystywana jest macierz SLA z lokalnym zapominaniem krawędzi. Zasada działania procedury aktualizacji szkicu ukazana jest w pseudokodzie \ref{alg:sg_sketch_update}. Argumentami wejściowymi jest macierz sąsiedztwa SLA $A$, aktualny szkic $S$, zbiór wierzchołków \textcolor{red}{indukujących} nową krawędź lub krawędzie $\Omega$, rząd $k$ oraz współczynnik $\alpha$. Zasadniczo, mechanizm jest podobny do \textcolor{red}{pierwszego generowania szkicu, ale w tym przypadku większość wartości jest już znana, a przeliczane są} zanurzenia tylko tych elementów, które znajdują się w sąsiedztwie nowych krawędzi. Pozwala to na znaczną oszczędność w kwestii czasu obliczeń. 

            \begin{algorithm}
                \caption{SGSketchUpdate($\tilde{A},S,\Omega,k,\alpha$)}\label{alg:sg_sketch_update}
                \uIf{$k > 2$}{
                    zaktualizuj $S(k-1)$ i zbiór $\Omega$: $SGSketchUpdate(\tilde{A},S,\Omega,k-1,\alpha)$\;
                    \ForEach{$r \in \Omega$}{
                        $\color{red} \tilde{V}^{r}(k) \gets \tilde{V}_{i}^{r} + \sum\limits_{n \in 	\Gamma(r)} \frac{\alpha}{L} \sum\limits_{j = 1}^{L} \mathbbm{1}_{[S_{j}^{n}(k - 1) = i]}, i \in \{1,2, \dots, D\}$\;
                        $\color{red} S_{j}^{r}(k) \gets \mathop{argmin}_{i \in \{1,2,\dots, D\}} \frac{-\log h_{j}(i)}{\tilde{V}_{i}^{r}(k)}, j \in \{1,2, \dots, L\}$\;
                    }
                }
                \ElseIf{$k = 2$}{
                    \ForEach{$r \in \Omega$}{
                        $\color{red} S_{j}^{r}(2) \gets  \mathop{argmin}_{i \in \{1,2,\dots, D\}} \frac{-\log h_{j}(i)}{\tilde{V}_{i}^{r}(2)}, j \in \{1,2, \dots, L\}$\;
                    }
                }
                \ForEach{$r \in \Omega$}{
                    $\Omega \gets \Omega \cup \Gamma(r)$\;
                }
            \end{algorithm}


\section{Porównanie wspieranych operacji i ich złożoności}
    TODO: Tabela będzie uzupełniona i rozbudowana

    \begin{table}[htbp]
        \centering
        \caption{Klasyfikacja metod}
        \begin{tabular}{l | c | c | c | c}
        \toprule
        \textbf{Metoda} & \textbf{Bezstratność} & \textbf{Złożoność pamięciowa} & \textbf{Grafy ważone} & \textbf{Etc.} \\
        \midrule
        \multicolumn{5}{c}{Matrix-based} \\
        \midrule
        TCM & Yes/No & O(|V|) & Yes/No & ... \\
        GSS & Yes/No & O(|V|) & Yes/No & ... \\
        Scube & Yes/No & O(|V|) & Yes/No & ... \\
        Horae & Yes/No & O(|V|) & Yes/No & ... \\
        \midrule
        \multicolumn{5}{c}{Embeddings} \\
        \midrule
        Method 3 & Yes/No & O(|V|) & Yes/No & ... \\
        Method 4 & Yes/No & O(|V|) & Yes/No & ... \\
        \midrule
        \multicolumn{5}{c}{MDL} \\
        \midrule
        Method 5 & Yes/No & O(|V|) & Yes/No & ... \\
        Method 6 & Yes/No & O(|V|) & Yes/No & ... \\
        \bottomrule
        \end{tabular}
    \end{table}

    \begin{table}[htbp]
        \centering
        \caption{Złożoność czasowa zapytań}
        \begin{tabular}{l | c | c | c | c}
        \toprule
        \textbf{Method} & \textbf{Lossless} & \textbf{Node query} & \textbf{Edge query} & \textbf{Etc.} \\
        \midrule
        \multicolumn{5}{c}{Matrix-based} \\
        \midrule
        Method 1 & Yes/No & O(|V|) & O(|V|) & ... \\
        Method 2 & Yes/No & O(|V|) & O(|V|) & ... \\
        \midrule
        \multicolumn{5}{c}{Embeddings} \\
        \midrule
        Method 3 & Yes/No & X & O(|V|) & ... \\
        Method 4 & Yes/No & X & X & ... \\
        \midrule
        \multicolumn{5}{c}{MDL} \\
        \midrule
        Method 5 & Yes/No & O(|V|) & O(|V|) & ... \\
        Method 6 & Yes/No & O(|V|) & O(|V|) & ... \\
        \bottomrule
        \end{tabular}
    \end{table}
\chapter{Przegląd literatury}

    Analiza wielkich grafów, zwłaszcza w ostatnich latach, przeżywa ogromny rozwój, budząc zainteresowanie grup badaczy z całego świata. Postępy w tej dziedzinie są naturalną odpowiedzią na potrzebę przetwarzania coraz większych zbiorów danych. Badanie interakcji w sieciach społecznościowych, zarządzanie ruchem internetowym, czy monitorowanie ruchu samochodowego to tylko niektóre z kluczowych w dzisiejszej rzeczywistości zastosowań. Grafy dobrze sprawdzają się jako modele do reprezentowania złożonych relacji między encjami, dzięki czemu odgrywają kluczową rolę w przetwarzaniu i wydobywaniu skondensowanych informacji ze strumieniowanych danych. Wybrane do tych celów algorytmy i metodologie w znacznym stopniu zależą od struktury badanych grafów, a także charakteru zapytań, które chcemy rozpatrywać. W zależności od wymagań dotyczących złożoności czasowej i pamięciowej, dokładności odpowiedzi, a także konkretnych informacji, na których zachowaniu nam zależy, inne metody mogą okazać się najlepszym wyborem. Przykładowo, odpowiedź na pytania o najkrótsze ścieżki między wierzchołkami może wymagać zapamiętania dodatkowych informacji o strukturze grafu, a więc potencjalnie użycia bardziej wyrafinowanego podejścia niż w przypadku zapytań wyłącznie o istnienie danej krawędzi. 

    W niniejszym przeglądzie literatury zagłębiamy się w sferę analizy dużych grafów ze szczególnym uwzględnieniem metod opartych na szkicach danych. Badamy ewoluujący krajobraz technik, algorytmów i aplikacji w tej dziedzinie, rzucając światło na metodologie stosowane w celu sprostania nieodłącznym wyzwaniom stawianym przez strumieniowe przesyłanie danych grafowych. Poprzez analizę najnowszych osiągnięć, staramy się zapewnić wgląd w znaczenie i potencjał analizy strumieni grafów w rozwiązywaniu złożonych zadań analitycznych w różnych dziedzinach, a także sformułować ogólne wnioski i wskazówki co do wyboru odpowiedniej metody do danego zastosowania. Dla lepszego ustrukturyzowania wiedzy omawiane algorytmy i struktury podzielone zostały na kilka kategorii, odpisanych dokładnie w dalszej części niniejszego rozdziału. Należy jednak pamiętać, że w niektórych przypadkach podział ten jest nieco umowny, gdyż różne podejścia i pomysły nierzadko przenikają się i inspirują wzajemnie, prowadząc do syntetycznych rozwiązań. 

\section{Streszczenia danych/Sketch synopses TODO: Potwierdzić nazewnictwo}
    Analiza strumieni danych jest szeroką dziedziną, nieograniczającą się oczywiście wyłącznie do danych grafowych. Istnieje wiele bardziej ogólnych, uniwersalnych metod, na których podstawie można budować rozwiązania bardziej wyspecjalizowane do konkretnych zadań. Doskonałym przykładem są streszczenia danych (\emph{ang. Sketch synopses}). Są to kompaktowe struktury zaprojektowane z myślą o wykorzystaniu ograniczonej ilości pamięci, umożliwiając jednocześnie aproksymację różnych statystyk i zapytań dotyczących strumienia danych, takich jak zliczanie elementów, wyznaczanie mediany, czy wykrywanie wartości odstających. Utrzymując mały szkic o stałym rozmiarze, struktury te umożliwiają analizę strumieni danych w czasie rzeczywistym bez konieczności przechowywania wszystkich elementów strumienia, co zapewnia wysoką wydajność nawet przy analizowaniu ogromnych strumieni.
    
    Istotną w kontekście budowy dalszych rozwiązań strukturą danych jest szkic Count-Min\cite{Cormode_Muthukrishnan_2005}. W przeciwieństwie do wielu proponowanych wcześniej streszczeń zaprojekotwanych do badania konkretnej statystyki stanowi on dość uniwersalną strukturę, oferując wsparcie dla zapytań o częstość występowania elementu, zakresu elementów oraz iloczyn skalarny dwóch strumieni. Z kolei na ich podstawie można budować bardziej skomplikowane zapytania. Co ważne, Count-Min, choć zwraca przybliżone wyniki, to gwarantuje spełnienie pewnych założeń odnośnie dokładności. Konkretnie, wyraża się ją zazwyczaj w kontekście definiowanych przez użytkownika parametrów $\epsilon, \delta$, a więc wymagamy, aby błąd względny w odpowiedzi na zapytanie mieścił się w zakresie współczynnika $\epsilon$ z prawdopodobieństwem $\delta$. Opiera się ona na zastosowaniu dwuwymiarowej tablicy o wymiarach $w \times d$, gdzie $w = \lceil \frac{e}{\epsilon} \rceil$ oraz $d = \lceil \ln \frac{1}{\delta} \rceil$. Nietrudno zauważyć, że zależą one od wymaganej dokładności. Struktura dalej wykorzystuje $d$ niezależnych funkcji haszujących, mapujących elementy ze strumienia na kolumny tablicy. Każdy z napływających elementów wiąże się więc z aktualizacją jednej komórki w każdym wierszu tablicy. Wtedy, przykładowo częstotliwość elementu możemy aproksymować jako minimum z wartości odpowiadających mu komórek. 

    Do innych wartych wzmianki metod możemy zaliczyć np. Lossy Counting\cite{Manku_Motwani_2012}, ukierunkowany na wskazywanie szczególnie często występujących elementów, czy AMS\cite{Alon_Matias_Szegedy_1996}, aproksymujący momenty częstotliwości (\emph{ang. momenty częstotliwości}).
    
    gSketch\cite{Zhao_Aggarwal_Wang_2011} stanowi jedną z pierwszych prób przeniesienia idei tych ogólnych metod do świata grafów. Obsługuje on proste zapytania dotyczących grafu, takie jak częstość występowania danej krawędzi w strumieniu lub gęstość wybranego podgrafu. Autorzy wychodzą od metody Count-Min, wykorzystując jej dwuwymiarową tablicę do składowania częstotliwości krawędzi. Dodatkowo, algorytm wykorzystuje próbkę testową do podziału zbioru krawędzi na podzbiory, bazując na ich częstotliwości w taki sposób, aby efektywnie wykorzystać dostępną pamięć, a następnie przetwarza właściwy strumień danych. To przetwarzanie wstępne daje cenny wgląd w charakterystykę grafu, ale należy pamiętać, że wyznaczanie reprezentacyjnej dla danego zbioru próbki nie zawsze jest trywialne. gSketch, podobnie jak Count-Min jest metodą stratną, a praktycznym wyzwaniem jest odpowiednie wyważenie parametrów tak, aby zachować balans między zużytą pamięcią a dokładnością wyników.
    
    Choć metody takie jak gSketch mogą efektywnie zapamiętywać informacje dotyczące częstotliwości występowania krawędzi, to tracą przy tym wiedzę o strukturze grafu. Przykładowo, trudno za ich pomocą odpowiedzieć, czy istnieje ścieżka między danymi dwoma wierzchołkami. Jedną z prób rozwiązania tego problemu jest struktura gMatrix\cite{Khan_Aggarwal_2016}. Wykorzystuje ona, podobnie jak gSketch, zasadę działania Min-Count. Jednak w tym wypadku tablica zliczająca elementy wzbogaciła się o trzeci wymiar. Konkretnie, długość i szerokość tablicy odpowiadają haszom wierzchołków, a jej głębokość związana jest ponownie z liczbą funkcji haszujących. Struktura może więc wspierać zapytania związane zarówno z krawędziami, jak i wierzchołkami. gMatrix wspierać również wykrywanie szczególnie często występujących krawędzi i wierzchołków, a więc tych, których częstotliwość przekracza dany parametr $F$. Wymaga to jednak, aby wybrane funkcje haszujące były odwracalne. Wtedy wystarczy wybrać komórki o odpowiednio dużych wartościach i obliczyć odwrotności haszy, aby odzyskać informacje o wierzchołkach. Podobnie, można rozważać osiągalność między wierzchołkami, wybierając krawędzie o częstotliwości występowania przekraczającej pewne $F$ i przetwarzając je używając tradycyjnych algorytmów. Zwłaszcza w przypadku grafów o nierównej gęstości, możemy w ten sposób znacznie ograniczyć liczbę badanych krawędzi, zachowując wciąż odpowiednie ograniczenia na prawdopodobieństwo błędu.

\section{MDL -- minimalna długość opisu}
    Kluczowym problemem w analizie wielkich grafów jest rozmiar danych. Zasadne wydaje się więc pytanie, czy sposób zapisu analizowanych grafów jest efektywny. W wielu przypadkach może się okazać, że sama próba zmiany modelu opisującego dane przynosi znaczne oszczędności w kwestii wykorzystanej pamięci. Metoda minimalnej długości opisu (MDL - emph{ang. Minimum Description Length}) koncentruje się na znalezieniu najprostszego modelu, który najlepiej opisuje strukturę grafu. Techniki oparte na MDL identyfikują wzorce i kompresują graf, wybierając model, który minimalizuje całkowitą długość opisu modelu i danych przy danym modelu, ułatwiając w ten sposób wydajne przechowywanie, przesyłanie i analizę dużych grafów. Bardziej formalnie, dla danych $D$ i rodziny dostępnych modeli $MF$ szukamy takiego modelu $M \in MF$, który minimalizuje $L(M) + L(D|M)$, gdzie $L(M)$ i $L(D|M)$ oznaczają odpowiednio długość opisu modelu $M$ oraz zakodowanych w nim danych $D$. Wiele metod opartych na MDL kompresuje dane w sposób bezstratny, co jest niewątpliwą zaletą tego modelu. 

    Istnieje wiele bezstratnych metod kompresujących grafy do reprezentacji o mniejszym narzucie pamięciowym. Jednak większość z nich zakładała działanie na tradycyjnej postaci grafu, gdzie  dane są skończone i znane na wejściu. Rzeczywistość analizy strumieni grafowych wymaga jednak bardziej elastycznego podejścia. Jedną z pierwszych inkrementacyjnych metod kompresji grafu jest MoSSo\cite{Ko_Kook_Shin_2020}. Reprezentacja wyjściowa tej metody składa się ze zbioru superwęzłów, a więc zbiorów wierzchołków oraz superkrawędzi. Każda taka superkrawędź oznacza połączenie wszystkich wierzchołków z danego superwęzła z wierzchołkami drugiego superwęzła. Dodatkowo, częścią zapisu jest także zbiór korekt krawędzi. Mają one postać pary zbiorów $C = (C^{+}, C^{-})$, oznaczających krawędzie, które należy dodać i usunąć, aby otrzymać prawdziwe dane. W ogólności aktualizowanie struktury dla nowych krawędzi sprowadza się do przemieszczania wierzchołków między superwęzłami w taki sposób, aby zminimalizować długość zapisu. Oczywiście sprawdzenie wszystkich możliwości byłoby kosztowne, dlatego autorzy zakładają sprawdzanie za każdym razem pewnego losowego zbioru potencjalnych wierzchołków do przemieszczania, co pozwala na znaczną oszczędność czasu, przy zachowaniu zadowalającego zużycia pamięci.

    W opisanym wyżej MoSSo losowy wybór potencjalnych zmian jest sterowany parametrami takimi jak prawdopodobieństwo utworzenia nowego superwęzła i liczba wierzchołków, których przesunięcie należy rozważyć w każdej rundzie. Jeśli wybrane wartości parametrów są zbyt małe lub zbyt wielkie, może to negatywnie wpływać na czas działania lub skuteczność kompresji. Jednak optymalne ich dobranie dla nieznanych wcześniej danych może być niemożliwe. Problem ten zauważają autorzy SGS\cite{Ma_Liu_Yang_Yang_Li_2022}. Jest to metoda bezparametryczna, w każdym kroku rozważająca przemieszczenie jedynie wierzchołków indukujących obecnie rozważaną krawędź $(u,v)$. Dodatkowo SGS opiera się na obserwacji, że w taki wypadku wystarczy rozważyć superwęzły bazując na wierzchołkach znajdujących się w odległości co najwyżej dwóch kroków od $u$ lub $v$. Następnie najlepsza zmiana jest wyznaczana zachłannie, bazując na funkcji podobieństwa sąsiedztwa wierzchołka oraz superwęzła.

    Przykładem nieco odmiennego podejścia może być z kolei metoda GS4\cite{Ashrafi-Payaman_Kangavari_Hosseini_Fander_2020}. O ile ona również wykorzystuje koncept grupowania wierzchołków w superwęzły, o tyle robi to, bazując zarówno na strukturze grafu, jak i pewnym zbiorze atrybutów wierzchołków. Jest to przydatne w przypadku, gdy wierzchołki grafu reprezentują bardziej złożone informacje. Przykładowo, w przypadku analizy portalów społecznościowych, wierzchołkiem może być użytkownik, a atrybutami jego imię, wiek, ulubione zwierzę z gromady stułbiopławów itd. Niektóre z tych danych mogą być ważniejsze niż inne. GS4 pozwala na ustalenie wag dla atrybutów, dzięki czemu jest rozwiązaniem bardziej elastycznym, pozwalającym na sterowanie tym, jakie cechy grafu będą zachowywane priorytetowo. W przeciwieństwie do omawianych wcześniej algorytmów GS4 jest jednak metodą stratną. Dla oszczędności czasu przechowywany graf nie jest aktualizowany dla każdej nowej krawędzi, a raczej w wypadku istotnych różnic.  

\section{Metody oparte na modyfikacji macierzy sąsiedztwa}
    Jednym z najbardziej popularnych i być może najprostszym koncepcyjnie sposobem reprezentacji grafu jest macierz sąsiedztwa. Jej wiersze i kolumny odpowiadają poszczególnym wierzchołkom, a w komórkach przechowywane są wagi krawędzi pomiędzy nimi, o ile takowe krawędzie istnieją. W przypadku grafów nieważonych może to być np. wartość logiczna indykująca istnienie krawędzi lub ustalona stała. Ten sposób reprezentacji ma niewątpliwe zalety takie jak prostota implementacji i, przede wszystkim, stały czas dostępu do wag krawędzi. Z tego powodu niezaskakujący jest pomysł zachowania ogólnej zasady działania macierzy sąsiedztwa, przy jednoczesnej próbie zmniejszenia jej rozmiaru.     

    Jedną z pierwszych realizacji tej idei jest struktura TCM\cite{Tang_Chen_Mitra_2016}. Ma ona postać macierzy o boku długości $m$, gdzie $m$ jest pewną stałą. Podobnie jak w klasycznej macierzy sąsiedztwa, w jej komórkach składowane są wagi krawędzi. Zasadniczą różnicą jest natomiast sposób wyznaczania rzędu i kolumny odpowiadających danej parze wierzchołków. Są one bowiem wyznaczane przez wynik funkcji haszującej $H : V \rightarrow [1..m]$. Czas obliczania hasza jest stały, a co za tym idzie, złożoność czasowa zapytań i dodawania nowych krawędzi również. Teoretyczna złożoność pamięciowa także jest stała i wynosi $O(m^2)$. W praktycznych zastosowaniach wybór $m$ zależy jednak często od liczby krawędzi i przejmuje się najczęściej $m$ rzędu $O(\sqrt{|V|})$. Dokładność rezultatów zależy od rozmiaru macierzy i może być niska ze względu na kolizje haszy. Łatwo zauważyć, że jeśli $m$ jest istotnie mniejsze od $|V|$ to może do nich dochodzić często, co powoduje traktowanie różnych krawędzi jako kolejnych instancji tego samego połączenia. Autorzy, świadomi tego ograniczenia, proponują zastosowanie kilku parami niezależnych funkcji haszujących i stworzenie na ich podstawie wielu szkiców grafu. Przykładowo, jeśli badaną zmienną jest suma wag kolejnych instancji krawędzi między danymi dwoma wierzchołkami, to algorytm może sprawdzić odpowiednie komórki dla wszystkich szkiców, a następnie zwrócić minimalną wartość. Podejście to pozwala na analizę większych grafów niż w przypadku pojedynczego szkicu, ale ostatecznie nie rozwiązuje całkowicie problemu. Użyteczność struktury TCM w bazowej formie jest dyskusyjna, stanowi ona jednak punkt wyjściowy dla bardziej zaawansowanych rozwiązań.

    Strukturą opartą na koncepcie podobnym do TCM jest \emph{Graph Stream Sketch} (GSS)\cite{Gou_Zou_Zhao_Yang_2019}. Celem autorów było stworzenie metody oferującej lepszą skalowalność dla wielkich grafów. Podobnie jak w TCM, funkcja haszująca mapuje zbiór wierzchołków na pewien mniejszy zbiór $M$-elementowy. Rozmiar macierzy jest natomiast równy $m$, $m < M$. Główną zmianą jest wprowadzenie dodatkowych cech opisujących wierzchołki. Na podstawie hasza $H(v)$ wyznaczany jest  podpis wierzchołka $f(v) (0 \leq f(v) < F)$, gdzie $M = m \times F$ i $f(v) = H(v)\%F$, a także adres $h(v) = \lfloor \frac{H(v)}{F} \rfloor$. Adresy służą do wyznaczania rzędu i kolumny komórek. Komórki te mają postać krotki lub, bardziej obrazowo, kubełka, w którym przechowywana jest para podpisów wierzchołków tworzących krawędź oraz kumulatywna waga krawędzi. Przechowywanie podpisów w komórkach pozwala zredukować ryzyko kolizji haszy. Łatwo bowiem zauważyć, że nawet jeśli dwa różne wierzchołki mają taki sam adres, to istnieje duża szansa, że ich podpisy są różne. Z tego względu nowa krawędź jest dodawana do kubełka tylko w wypadku, gdy jest on pusty lub gdy istniejące w nim podpisy są zgodne z podpisami wierzchołków krawędź tą tworzących. W przeciwnym przypadku jest ona zapisywana w dodatkowym buforze, mającym postać listy sąsiedztwa pełnych haszy. Pozwala on na dodawanie nowych krawędzi z niskim ryzykiem kolizji, nawet jeśli sama macierz jest już zapełniona. Należy natomiast zauważyć, że część macierzowa struktury jest bardziej efektywna czasowo, oferując stały czas odpowiedzi na zapytanie, podczas gdy dla bufora jest on liniowy względem liczby wierzchołków. Dokładność odpowiedzi w części macierzowej zależy od długości podpisów. Potencjalnym problemem GSS jest niskie wykorzystanie pamięci w macierzy. Przy kolizji adresów nowe krawędzie mogą trafiać do bufora, mimo, że w samej macierzy pozostaje wiele pustych komórek. Aby temu zaradzić, autorzy proponują haszowanie krzyżowe (\emph{ang. square-hashing}). Zakłada ono obliczanie dla każdego wierzchołka sekwencji niezależnych adresów. Podczas wstawiania nowych krawędzi algorytm sprawdza nie jedną komórkę macierzy, a kilka, zgodnie z sekwencją adresów i wybiera pierwszą spełniającą wymagania co do zgodności podpisów. Istnieje probabilistyczne ograniczenie na błąd względny zapytań postaci $Pr(\tilde{f}(s,d) - f(s,d) / \bar{w} > \delta) \leq \frac{|E|}{\delta m^2 4^{f}}$, gdzie $\tilde{f}(s,d)$ jest zwróconą sumą wag krawędzi $(s,d)$, $f(s,d)$ jej rzeczywistą wartością, $\bar{w}$ średnią wagą krawędzi, a $f$ długością podpisu.

    Większość struktur służących podsumowujących strumieniowane grafy nie przechowuje informacji o czasie wystąpienia krawędzi. Nie wspierają one więc zapytań z zakresem czasowym, a więc np., czy dana krawędź wystąpiła w zakresie $[t, t + L)$. Tego typu zapytania mogą być kluczowe np. w przypadku analizy danych dotyczących rozprzestrzeniania się wirusów (TODO: Citation needed). Problem ten podejmuje praca proponująca strukturę Horae\cite{Chen_Zhou_Chen_Xiao_Jin_Li_2022}. W jej wypadku krawędź $e_i = (<s_i, d_i>, w_i, t_i)$ jest wstawiana do komórki o adresie $(h(s_i | \gamma(t_i)), h(d_i | \gamma(t_i)))$, gdzie $\gamma(t_i) = \lfloor \frac{t_i}{gl} \rfloor$ i $gl$ jest długością przedziałów czasowych. Intuicyjnie, zapytanie o pojawienie się krawędzi w zakresie czasowym $[T_b, T_e]$ moze być transformowane w sekwencję zapytań o pojedyncze zakresy, których wyniki są sumowane, a więc $Q([T_b, T_e]) = Q([T_b]), Q([T_{b+1}]), \dots Q([T_e])$. Jednak dla takiego algorytmu złożoność czasowa jest liniowa względem liczby zakresów. Autorzy starają sie poprawić ten aspekt, zauważajac, iż przedział długości $L$ może zostać zdekomponowany do co najwyżej $2\log{L}$ podprzedziałów podsiadających dwie szczególne cechy. Po pierwsze, wszystkie zakresy czasowe w danym podprzedziale mają wspólny prefiks binarny. Po drugie, prefiksy różnych podprzedziałów mają różne długości. Z tego względu Horae zapamiętuje $O(\log(T))$ identycznych skompresowanych macierzy, gdzie $T$ jest liczbą rozróżnialnych zakresów czasowych. Każda z nich jest utożsamiana z jedną warstwą struktury. Warstwy odpowiadają z kolei różnym długościom prefiksów. Dzięki temu zamiast wykonywać liniową względem długości przedziału czasowego liczbę zapytań, wystarczy zdekomponować przedział na podprzedziały i na ich podstawie wykonać co najwyżej jedno zapytanie na warstwę. 
    
    Metody oparte na macierzach w większości przypadków nie czynią założeń co do struktury grafu. Takie ogólne podejście oczywiście zapewnia wysoką uniwersalność, jednak w niektórych przypadkach może być nieefektywne. Przykładowo, jeśli wierzchołki w grafie są mocno zróżnicowane pod względem stopnia, a więc bardziej obrazowo, da się wyróżnić obszary gęste i rzadkie w grafie, to kolizje haszy mogą zdarzać się często. Struktura Scube\cite{Chen_Zhou_Chen_Jin_2022} używa probabilistycznego zliczania do identyfikacji wierzchołków wysokiego stopnia. Przeznaczane jest dla nich więcej kubełków w macierzy niż dla wierzchołków o niskich stopniach, co pozwala bardziej efektywnie zarządzać zapełnieniem macierzy. 

    Metody takie jak GSS czy Horae, choć często dają przyzwoite wyniki przy odpowiednim dobraniu parametrów do badanego grafu, to ostatecznie cierpią z uwagi na ograniczoną skalowalność. Jedną z prób odpowiedzi na ten problem jest struktura AUXO\cite{Jiang_Chen_Jin_2023}. Korzysta ona z macierzy przechowujących podpisy wierzchołków, podobnie jak GSS. Jednak, zamiast wstawiać nadmiarowe krawędzie do bufora o liniowym czasie dostępu, AUXO wykorzystuje wiele macierzy ustawionych w strukturę drzewa. Konkretnie, jest to binarne lub czwórkowe drzewo prefiksowe, w którego strukturę zaszyte zostały prefiksy podpisów wierzchołków. W ten sposób na każdym kolejnym poziomie drzewa podpisy przechowywane w komórkach mogą być coraz krótsze, gdyż informacja ta jest wbudowana w kształt struktury. Pozwala to osiągnąć logarytmiczny względem liczby krawędzi czas odpowiedzi na zapytania. Warto zauważyć, że złożoność pamięciowa jest ograniczona przez długość podpisów, która wyznacza maksymalną głębokość drzewa. Niemniej jednak liczba możliwych do przetworzenia krawędzi jest eksponencjalna w stosunku do liczby bitów podpisu, więc stosunkowo łatwo można dobrać wystarczające wartości. W praktycznych zastosowaniach autorzy wskazują, nieco niefortunnie, na złożoność pamięciową zbliżoną asymptotycznie do $O(|E|(1 - \log(E)))$. Jak widać, AUXO osiąga efektywność pamięciową i skalowalność kosztem zwiększenia złożoności czasowej, co może być potencjalną wadą tego rozwiązania.

\section{Graph Spanners}
    TODO: np. \cite{Elkin_Trehan_2022}, QbS \cite{Wang_Wang_Koehler_Lin_2021}

\section{Zanurzenia TODO: potwierdzić nazewnictwo}
    Kolejną, dość rozległą i rozwijającą się metodologią w dziedzinie analizy grafów są zanurzenia grafów (\emph{ang. graph embeddings}). Nazwa odnosi się do reprezentowania wierzchołków przez wektory cech, a więc bardziej obrazowo, zanurzania grafu w niskowymiarowych przestrzeniach wektorowych. Wektory te mogą zachowywać między innymi kluczowe informacje topologiczne związane z połączeniami danego wierzchołka z jego sąsiadami. Przyjmują one najczęściej wartości rzeczywiste, a podobieństwo między wektorami może być mierzone na różne sposoby, np.  za pomocą podobieństwa cosinusów (\emph{ang. cosine similarity}) lub odległości Hamminga\cite{Lian_Zheng_Zheng_Ge_Cao_Tsang_Xie_2018}. Co istotne, tego typu reprezentacja pozwala na łatwe zastosowanie uczenia maszynowego do analizy danych. W ogólności zanurzenia są stosowane szczególnie często w takich zadaniach jak klasyfikacja węzłów, przewidywanie połączeń między nimi oraz rekonstrukcja grafu. Wśród algorytmów opartych na tej metodologii możemy wyróżnić kilka unikalnych podkategorii, cechujących się odmiennymi podejściami do tego, jak cechy są wybierane i przetwarzane. Praca \cite{Yang_Qu_Hussein_Rosso_Cudré-Mauroux_Liu_2023} stanowi stosunkowo aktualny i rozbudowany przegląd tego typu metod, ukazując przy okazji wpływ hiperparametrów na uzyskiwane wyniki, zwłaszcza dla metod opartych na faktoryzacji i próbkowaniu. Autorzy proponują też uogólnione techniki wykorzystujące te metodologie i dzielą się pewnymi wskazówkami co do wyboru hiperparametrów dla konkretnych scenariuszy.  

    \subsection{Faktoryzacja}
        Metody opierające się na faktoryzacji, zakładają dekompozycję reprezentacji macierzowej grafu w taki sposób, aby uchwycić ukryte relacje między węzłami w przestrzeni o niższym wymiarze. Ma to na celu zachowanie właściwości strukturalnych grafu, takich jak bliskość węzłów i struktura sąsiedztwa. Jednak koszt obliczeniowych faktoryzacji macierzy może być bardzo wysoki w przypadku  dużych grafów, zarówno w kontekście wykorzystania pamięci, jak i czasu obliczeń.
        TODO: np. GraRep\cite{Cao_Lu_Xu_2015}, HOPE\cite{Ou_Cui_Pei_Zhang_Zhu_2016}, NetMF\cite{Qiu_Dong_Ma_Li_Wang_Tang_2018}, ProNE\cite{Zhang_Dong_Wang_Tang_Ding_2019} 

    \subsection{Próbkowanie}
        Techniki oparte na próbkowaniu koncentrują się na wyborze reprezentatywnego zbioru par węzłów z grafu wejściowego, w celu uchwycenia jego podstawowych właściwości strukturalnych. Na ich podstawie określony model uczy się reprezentowania wierzchołków poprzez optymalizację stochastyczną. Zapewnienie zadowalającej jakości reprezentacji wyjściowej wymaga często próbkowania znacznej liczby par węzłów, a zatem wymaga dużych zasobów obliczeniowych, w szczególności czasu procesora.
            
        Znanym przykładem tego typu techniki jest DeepWalk\cite{Perozzi_Al-Rfou_Skiena_2014}, wywodzący się, jak sama nazwa wskazuje, z uczenia głębokiego (\emph{ang. deep learning}). Wykorzystuje on sekwencję krótkich, losowych spacerów po wierzchołkach grafu do zapamiętywania tzw. reprezentacji społecznej wierzchołków grafu, zawierającej informacje o podobieństwie sąsiedztwa wierzchołków i przynależności do pewnych grup. DeepWalk wykorzystuje algorytm SkipGram do efektywnego wyznaczania wektorów cech. Co ciekawe, można na ten schemat działanie patrzeć jak na uogólnienie metod przetwarzania języka naturalnego do przetwarzania języka losowych spacerów, traktowanych jako zdania. 
        
        Node2Vec\cite{Grover_Leskovec_2016} rozwija ideę DeepWalk, wprowadzając dodatkowe parametry sterujące spacerem. Pozwala to na wybranie, jak szybko spacer może oddalać się od startowego wierzchołka, efektywnie regulując, jak bardzo spacer zbliżony jest do przeszukiwanie wszerz lub przeszukiwania wgłąb. Odpowiednie regulowanie tych parametrów sprzyja to lepszemu poznaniu struktury grafu, która mogłaby być trudna do uchwycenia przy zwykłym spacerze losowym. 
        
        LINE\cite{Tang_Qu_Wang_Zhang_Yan_Mei_2015}, SDNE\cite{Wang_Cui_Zhu_2016}, VERSE\cite{Tsitsulin_Mottin_Karras_Müller_2018} 

    \subsection{Metody oparte na sieciach neuronowych}
        TODO: np. DNE, DVNE, GCN, and GraphS-AGE

    \subsection{Szkice}
        TODO: np. NH-MF, NetHash, \#GNN, NodeSketch\cite{Yang_Rosso_Li_Cudre-Mauroux_2019}, SGSketch\cite{Yang_Qu_Yang_Wang_Cudre-Mauroux_2022}.

\section{Porównanie wspieranych operacji i ich złożoności}
    TODO: Tabela będzie uzupełniona i rozbudowana

    \begin{table}[htbp]
        \centering
        \caption{Your Table Title Here}
        \begin{tabular}{l | c | c | c | c}
        \toprule
        \textbf{Method} & \textbf{Lossless} & \textbf{Node query} & \textbf{Edge query} & \textbf{Etc.} \\
        \midrule
        \multicolumn{5}{c}{Matrix-based} \\
        \midrule
        Method 1 & Yes/No & O(|V|) & O(|V|) & ... \\
        Method 2 & Yes/No & O(|V|) & O(|V|) & ... \\
        \midrule
        \multicolumn{5}{c}{Embeddings} \\
        \midrule
        Method 3 & Yes/No & X & O(|V|) & ... \\
        Method 4 & Yes/No & X & X & ... \\
        \midrule
        \multicolumn{5}{c}{MDL} \\
        \midrule
        Method 5 & Yes/No & O(|V|) & O(|V|) & ... \\
        Method 6 & Yes/No & O(|V|) & O(|V|) & ... \\
        \bottomrule
        \end{tabular}
    \end{table}